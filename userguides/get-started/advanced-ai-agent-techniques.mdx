---
title: 'Advanced AI Agent Techniques'
description: 'Advanced guide for running AI agents with your own LLMs using xpander SDK'
---

# Advanced AI Agent Techniques

This guide covers advanced techniques for running AI agents with your own LLMs using the xpander SDK. Make sure you've completed the [Bring Your Own LLM](/userguides/get-started/bring-your-own-llm) guide first.

# Local Function Implementation

## File Operations
Building upon the basic local tools setup, here's how to implement secure file operations:

```python
import json
import xml.etree.ElementTree as ET
import os
import csv
from io import StringIO

def write_file(file_path, content, fileType):
    """
    Secure file writing with format-specific handling
    """
    try:
        if not is_within_current_directory(file_path):
            return {"error": "Access denied: Path outside current directory"}
            
        if fileType == "json":
            try:
                parsed_content = json.loads(content) if isinstance(content, str) else content
                with open(file_path, "w") as file:
                    json.dump(parsed_content, file, indent=2)
            except json.JSONDecodeError:
                return {"error": "Invalid JSON content"}
                
        elif fileType == "csv":
            try:
                content = list(csv.reader(StringIO(content))) if isinstance(content, str) else content
                with open(file_path, "w", newline="") as file:
                    writer = csv.writer(file)
                    writer.writerows(content)
            except Exception as e:
                return {"error": f"CSV write failed: {str(e)}"}
                
        elif fileType == "xml":
            try:
                root = ET.Element("root")
                ET.SubElement(root, "content").text = str(content)
                tree = ET.ElementTree(root)
                tree.write(file_path, encoding="unicode", xml_declaration=True)
            except Exception as e:
                return {"error": f"XML write failed: {str(e)}"}
                
        elif fileType in ["txt", "text"]:
            with open(file_path, "w") as file:
                file.write(str(content))
        else:
            return {"error": f"Unsupported format: {fileType}"}
            
        return {"success": f"File written to {file_path}"}
        
    except Exception as e:
        return {"error": f"Write failed: {str(e)}"}
```

# Enhanced Memory Management

Implement a robust conversation manager for better context handling:

```python
class ConversationManager:
    def __init__(self, system_prompt="", max_history=50):
        self.memory = []
        self.number_of_calls = 1
        self.max_history = max_history
        if system_prompt:
            self.add_system_message(system_prompt)
    
    def add_message(self, role, content, **kwargs):
        """Add message with automatic history management"""
        self.memory.append({"role": role, "content": content, **kwargs})
        if len(self.memory) > self.max_history:
            # Keep system prompt and trim oldest messages
            self.memory = [self.memory[0]] + self.memory[-self.max_history+1:]
    
    def add_tool_result(self, result, tool_call_id):
        """Add tool execution result"""
        self.add_message(
            role="tool",
            content=json.dumps(result),
            tool_call_id=tool_call_id
        )
    
    def get_messages(self):
        return self.memory
```

# Advanced Agent Patterns

## Multi-LLM Pipeline
Combine different LLMs for optimal performance:

```python
class MultiLLMAgent:
    def __init__(self, agent, openai_client, ollama_client):
        self.agent = agent
        self.openai = openai_client
        self.ollama = ollama_client
        self.conversation = ConversationManager()
    
    def process_with_openai(self, prompt):
        """Use OpenAI for complex reasoning"""
        response = self.openai.chat.completions.create(
            model="gpt-4",
            messages=self.conversation.get_messages(),
            tools=self.agent.get_tools()
        )
        return response.choices[0].message
    
    def process_with_ollama(self, prompt):
        """Use Ollama for local tasks"""
        response = self.ollama.chat(
            model="qwen2.5-coder:14b",
            messages=self.conversation.get_messages(),
            tools=self.agent.get_tools()
        )
        return response['message']
    
    async def process_task(self, task):
        """Route tasks to appropriate LLM"""
        if self._requires_local_execution(task):
            return await self._process_local_task(task)
        return await self._process_cloud_task(task)
```

## Robust Error Handling

```python
class ToolExecutor:
    def __init__(self, agent):
        self.agent = agent
        self.max_retries = 3
    
    async def execute_tool(self, tool_call):
        """Execute tool with retries and error handling"""
        for attempt in range(self.max_retries):
            try:
                if tool_call.type == ToolCallType.LOCAL:
                    return await self._execute_local_tool(tool_call)
                return await self._execute_cloud_tool(tool_call)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    return {"error": f"Tool execution failed: {str(e)}"}
                await asyncio.sleep(1 * (attempt + 1))
    
    async def _execute_local_tool(self, tool_call):
        """Handle local tool execution"""
        if not self._validate_tool_call(tool_call):
            return {"error": "Invalid tool call parameters"}
            
        body_params = tool_call.payload.get('bodyParams', {})
        if "write-file" in tool_call.name:
            return write_file(
                file_path=body_params.get('fileName'),
                content=body_params.get('fileContent'),
                fileType=body_params.get('fileType')
            )
```

# Complete Advanced Implementation

Here's how to put it all together:

```python
async def run_advanced_agent():
    # Initialize components
    agent = xpander_client.agents.get(agent_id=xpanderAgentID)
    conversation = ConversationManager(
        system_prompt="You are an advanced AI assistant with tool access. "
        "Add ##FINAL ANSWER## to end."
    )
    tool_executor = ToolExecutor(agent)
    
    # Add local tools
    agent.add_local_tools(localTools)
    
    # Process user request
    conversation.add_message(
        role="user",
        content="Create four hello world files with different formats"
    )
    
    while True:
        # Get LLM response
        llm_response = await process_with_llm(conversation.get_messages())
        conversation.add_message(role="assistant", content=llm_response)
        
        # Execute tools
        if tool_calls := extract_tool_calls(llm_response):
            for tool_call in tool_calls:
                result = await tool_executor.execute_tool(tool_call)
                conversation.add_tool_result(result, tool_call.id)
        
        # Check for completion
        if "##FINAL ANSWER##" in llm_response:
            break
```

# Performance Optimization

1. **Batch Processing**: Group similar tool calls
2. **Caching**: Implement response caching for repeated operations
3. **Async Execution**: Use asyncio for concurrent tool execution
4. **Memory Optimization**: Implement sliding window for conversation history

# Security Best Practices

1. **Input Validation**: Sanitize all inputs before processing
2. **Path Traversal Prevention**: Restrict file operations to safe directories
3. **Rate Limiting**: Implement rate limiting for tool calls
4. **Error Handling**: Never expose internal errors to responses

Need help? Visit our [Discord community](https://discord.gg/xpander) or [documentation](https://docs.xpander.ai). 
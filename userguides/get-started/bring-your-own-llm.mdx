---
title: 'Bring Your Own LLM'
description: 'Guide to using xpander SDK with your own LLM deployment'
---

While xpander.ai offers a serverless solution where you can use AI agents without managing LLM infrastructure, you might want to use your own LLM deployment for:

- Running models locally for lower latency
- Using specific model versions or custom fine-tuned models
- Implementing local function calling
- Maintaining full control over your LLM stack
- Handling sensitive data locally

This guide shows how to integrate your LLM with xpander's tools and local functions.

<Steps>
  <Step title="Prerequisites">
    1. Install Python 3.8 or higher
    2. xpander account and API key
    3. Your chosen LLM setup (OpenAI API or Ollama local deployment)
  </Step>

  <Step title="Installation">
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # On Windows: .venv\Scripts\activate
    pip install xpander-sdk openai ollama python-dotenv
    ```
  </Step>

  <Step title="Environment Setup">
    ```env
    XPANDER_API_KEY="your_xpander_key"
    XPANDER_AGENT_ID="your_agent_id"
    OPENAI_API_KEY="your_openai_key"  # If using OpenAI
    ```
  </Step>
</Steps>

# Choose Your LLM Provider

## OpenAI Integration

```python
from xpander_sdk import XpanderClient
from openai import OpenAI
import json

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
openai_client = OpenAI(api_key=openAIKey)

# Initialize conversation
memory = []
memory.append({
    "role": "system",
    "content": "You are a helpful assistant with function calling and tool access. Add ##FINAL ANSWER## to end."
})

while True:
    llm_response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=memory,
        tools=agent.get_tools()    
    )
    # ... rest of the loop implementation
```

## Local Ollama Integration

```python
from xpander_sdk import XpanderClient, LLMProvider, ToolCallType
import ollama
import json

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
ollama_client = ollama.Client()

# Initialize conversation
memory = []
memory.append({
    "role": "system",
    "content": "You are a helpful assistant with function calling and tool access. Add ##FINAL ANSWER## to end."
})

while True:
    llm_response = ollama_client.chat(
        model="qwen2.5-coder:14b",
        messages=memory,
        tools=agent.get_tools()    
    )
    # ... rest of the loop implementation
```

# Adding Local Functions

When using your own LLM, you can add local functions for file operations or other system interactions:

```python
# Define local tools
localTools = [
    {
        "type": "function",
        "function": {
            "name": "write-file",
            "description": "Writes content to a file in current directory",
            "parameters": {
                "type": "object",
                "properties": {
                    "bodyParams": {
                        "type": "object",
                        "properties": {
                            "fileName": {"type": "string"},
                            "fileContent": {"type": "string"},
                            "fileType": {"type": "string"}
                        },
                        "required": ["fileName", "fileContent", "fileType"]
                    }
                },
                "required": ["bodyParams"]
            }
        }
    }
]

# Add to your agent
agent.add_local_tools(localTools)
```

# Comparison with Serverless Option

| Feature | Bring Your Own LLM | xpander Serverless |
|---------|-------------------|-------------------|
| Infrastructure | Self-managed | Fully managed |
| Local Functions | Supported | Not available |
| Model Control | Full control | Pre-configured |
| Setup Required | More complex | Minimal |
| Cost Structure | Pay LLM provider | Pay per use |
| Latency | Can be lower | Network dependent |

# When to Choose Each Option

## Use Bring Your Own LLM When:
- You need local function execution
- You want to use specific model versions
- You require lower latency
- You have existing LLM infrastructure
- You need full control over the stack

## Use xpander Serverless When:
- You want minimal setup and maintenance
- You don't need local functions
- You prefer managed infrastructure
- You want automatic scaling
- You need quick deployment

# Complete Implementation Example

Here's a full example using Ollama with local functions:

```python
from xpander_sdk import XpanderClient, LLMProvider, ToolCallType
import ollama
import json

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
ollama_client = ollama.Client()

# Add local tools
agent.add_local_tools(localTools)

# Initialize conversation
memory = []
memory.append({
    "role": "system",
    "content": "You are a helpful assistant with function calling and tool access. "
    "Pay attention to use write-file and read-file carefully with bodyParams, fileName, "
    "fileContent and fileType. Filetype can be only txt, csv, json or xml. "
    "Add ##FINAL ANSWER## to end."
})

memory.append({
    "role": "user",
    "content": "Create four hello world files, one CSV, one JSON and one XML and one text."
})

number_of_calls = 1

while True:
    llm_response = ollama_client.chat(
        model="qwen2.5-coder:14b",
        messages=memory,
        tools=agent.get_tools()    
    )
    
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    memory.append(llm_response['message'])
    
    if llm_response['message'].get('tool_calls'):
        tools_to_run = XpanderClient.extract_tool_calls(
            llm_response=llm_response,
            llm_provider=LLMProvider.OLLAMA
        )
        
        for tool_to_run in tools_to_run:
            if tool_to_run.type == ToolCallType.LOCAL:
                function_name = tool_to_run.name
                body_params = tool_to_run.payload.get('bodyParams', {})
                
                if "write-file" in function_name:
                    result = write_file(
                        file_path=body_params.get('fileName'),
                        content=body_params.get('fileContent'),
                        fileType=body_params.get('fileType')
                    )
                memory.append({
                    "role": "tool",
                    "content": json.dumps(result),
                    "tool_call_id": function_name
                })
            else:
                tool_response = agent.run_tool(tool_calls=tools_to_run)
                memory.append({
                    "role": "tool",
                    "content": tool_response.result,
                    "tool_call_id": tool_response.tool_call_id
                })
    
    if "##FINAL ANSWER##" in llm_response['message'].get('content', ''):
        break
        
    number_of_calls += 1

print(llm_response['message']['content'])
```

# Best Practices

1. **Error Handling**: Implement robust error handling for both LLM and tool calls
2. **Memory Management**: Properly manage conversation context
3. **Security**: Validate all inputs and file operations
4. **Monitoring**: Log LLM and tool performance metrics
5. **Testing**: Test with different prompts and tool combinations

Need help? Visit our [Discord community](https://discord.gg/xpander) or [documentation](https://docs.xpander.ai). 
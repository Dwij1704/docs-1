---
title: 'Bring Your Own LLM'
description: 'Guide to using xpander SDK with your own LLM deployment'
---

While xpander.ai offers a serverless solution where you can use AI agents without managing LLM infrastructure, you might want to use your own LLM deployment for:

- Running models locally for lower latency
- Using specific model versions or custom fine-tuned models
- Implementing local function calling
- Maintaining full control over your LLM stack
- Handling sensitive data locally

<Steps>
  <Step title="Choose Your LLM Provider">
    To integrate your LLM with xpander's tools and local functions, you'll need to select **one** LLM provider and model combination. Import the necessary packages and initialize your client:

    <CodeGroup>
    ```python OpenAI
    from xpander_sdk import XpanderClient
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    load_dotenv()

    # Initialize credentials
    xpanderAPIKey = os.environ.get("XPANDER_API_KEY","")
    xpanderAgentID = os.environ.get("XPANDER_AGENT_ID", "")
    openaiAPIKey = os.environ.get("OPENAI_API_KEY", "")

    # Initialize OpenAI client
    openai_client = OpenAI(api_key=openaiAPIKey)

    # Initialize xpander client and get agent
    xpander_client = XpanderClient(api_key=xpanderAPIKey)
    agent1 = xpander_client.agents.get(agent_id=xpanderAgentID)
    ```

    ```python Amazon Bedrock
    from xpander_sdk import XpanderClient
    import boto3
    from dotenv import load_dotenv
    import os
    load_dotenv()

    # Initialize credentials
    xpanderAPIKey = os.environ.get("XPANDER_API_KEY","")
    xpanderAgentID = os.environ.get("XPANDER_AGENT_ID", "")

    session = boto3.Session(profile_name='your-profile')  # or use default credentials
    bedrock_client = session.client('bedrock-runtime', region_name='us-west-2')

    # Initialize xpander client and get agent
    xpander_client = XpanderClient(api_key=xpanderAPIKey)
    agent1 = xpander_client.agents.get(agent_id=xpanderAgentID)
    ```

    ```python Nvidia NIM
    from xpander_sdk import XpanderClient
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    load_dotenv()

    # Initialize credentials
    xpanderAPIKey = os.environ.get("XPANDER_API_KEY","")
    xpanderAgentID = os.environ.get("XPANDER_AGENT_ID", "")
    nvidiaAPIKey = os.environ.get("NVIDIA_API_KEY", "")

    # Initialize NVIDIA NIM client
    nvidia_client = OpenAI(
        base_url="https://integrate.api.nvidia.com/v1",
        api_key=nvidiaAPIKey
    )

    # Initialize xpander client and get agent
    xpander_client = XpanderClient(api_key=xpanderAPIKey)
    agent1 = xpander_client.agents.get(agent_id=xpanderAgentID)
    ```

    ```python Ollama
    from xpander_sdk import XpanderClient
    import ollama
    from dotenv import load_dotenv
    import os
    load_dotenv()

    # Initialize credentials
    xpanderAPIKey = os.environ.get("XPANDER_API_KEY","")
    xpanderAgentID = os.environ.get("XPANDER_AGENT_ID", "")

    # Initialize Ollama client
    ollama_client = ollama.Client()

    # Initialize xpander client and get agent
    xpander_client = XpanderClient(api_key=xpanderAPIKey)
    agent1 = xpander_client.agents.get(agent_id=xpanderAgentID)
    ```
    </CodeGroup>
  </Step>

  <Step title="Get Tools to the LLM">
    Configure your LLM client with the appropriate tools based on your provider:

    <CodeGroup>
    ```python OpenAI
    llm_response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=memory,
        tools=agent1.get_tools(llm_provider=LLMProvider.OPENAI)
    )
    ```

    ```python Amazon Bedrock
    # Get tools configuration for Bedrock
    tool_config = {
        "tools": agent1.get_tools(llm_provider=LLMProvider.AMAZON_BEDROCK)
    }

    # Prepare message format
    messages = [{
        "role": "user",
        "content": [{"text": prompt}]
    }]
    
    # Make API call to Bedrock
    llm_response = bedrock_client.converse(
        modelId="anthropic.claude-3-5-sonnet-20241022-v2:0",
        messages=messages,
        toolConfig=tool_config
    )
    ```

    ```python Ollama
    llm_response = ollama_client.chat(
        model="qwen2.5-coder",
        messages=memory,
        tools=agent1.get_tools(llm_provider=LLMProvider.OLLAMA)
    )
    ```

    ```python Nvidia NIM
    llm_response = nvidia_nim_client.chat.completions.create(
        model="meta/llama-3.1-70b-instruct",
        messages=memory,
        tools=agent1.get_tools(llm_provider=LLMProvider.NVIDIA_NIM)
    )
    ```
    </CodeGroup>
  </Step>

  <Step title="Run Tools for the LLM">
    Process the LLM response and execute the tools:

    <CodeGroup>
    ```python OpenAI
    tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.OPENAI)
    tool_responses = agent1.run_tools(tools_to_run)
    for response in tool_responses:
        print({
            "function_name": response.function_name,
            "status_code": response.status_code,
            "result": response.result,
            "payload": response.payload,
            "tool_call_id": response.tool_call_id
        })
    ```

    ```python Amazon Bedrock
    tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response, llm_provider=LLMProvider.AMAZON_BEDROCK)
    tool_responses = agent1.run_tools(tools_to_run)
    for response in tool_responses:
        print({
            "function_name": response.function_name,
            "status_code": response.status_code,
            "result": response.result,
            "payload": response.payload,
            "tool_call_id": response.tool_call_id
        })
    ```

    ```python Ollama
    tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response, llm_provider=LLMProvider.OLLAMA)
    tool_responses = agent1.run_tools(tools_to_run)
    for response in tool_responses:
        print({
            "function_name": response.function_name,
            "status_code": response.status_code,
            "result": response.result,
            "payload": response.payload,
            "tool_call_id": response.tool_call_id
        })
    ```

    ```python Nvidia NIM
    tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.NVIDIA_NIM)
    tool_responses = agent1.run_tools(tools_to_run)
    for response in tool_responses:
        print({
            "function_name": response.function_name,
            "status_code": response.status_code,
            "result": response.result,
            "payload": response.payload,
            "tool_call_id": response.tool_call_id
        })
    ```
    </CodeGroup>
  </Step>

  <Step title="Understanding extract_tool_calls">
    The `extract_tool_calls` method is a crucial utility that handles the conversion between different LLM function/tool calling formats. Here's how it works:

    1. **Purpose**: It standardizes various LLM response formats into a unified format that xpander can process:
    ```python
    # Example of how extract_tool_calls processes different formats
    # OpenAI format
    openai_response = {
        "tool_calls": [
            {
                "function": {"name": "get_weather", "arguments": '{"location": "London"}'}
            }
        ]
    }

    # Bedrock/Claude format
    bedrock_response = {
        "function_calls": [
            {"name": "get_weather", "arguments": {"location": "London"}}
        ]
    }

    # Both get converted to a standardized internal format
    standardized_calls = [
        {
            "name": "get_weather",
            "arguments": {"location": "London"}
        }
    ]
    ```

    2. **Provider-Specific Handling**: The method automatically detects and processes different response structures:
    ```python
    # The llm_provider parameter tells extract_tool_calls how to parse the response
    tools_to_run = XpanderClient.extract_tool_calls(
        llm_response=llm_response,
        llm_provider=LLMProvider.OPENAI  # or AMAZON_BEDROCK, OLLAMA, NVIDIA_NIM
    )
    ```

    3. **Automatic API Translation**: Once standardized, these tool calls can be executed uniformly:
    ```python
    # The standardized format allows consistent execution across providers
    tool_responses = agent1.run_tools(tools_to_run)
    
    # Each tool_response contains the result of the API call
    # Example response:
    # {
    #     "name": "get_weather",
    #     "result": {"temperature": 20, "condition": "sunny"}
    # }
    ```
  </Step>
</Steps>

Need help? Visit our [Discord community](https://discord.gg/xpander) or [documentation](https://docs.xpander.ai). 
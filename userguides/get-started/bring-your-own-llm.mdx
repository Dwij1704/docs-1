---
title: 'Bring Your Own LLM'
description: 'Guide to using xpander SDK with your own LLM deployment'
---

While xpander.ai offers a serverless solution where you can use AI agents without managing LLM infrastructure, you might want to use your own LLM deployment for:

- Running models locally for lower latency
- Using specific model versions or custom fine-tuned models
- Implementing local function calling
- Maintaining full control over your LLM stack
- Handling sensitive data locally

<Steps>
  <Step title="Choose Your LLM Provider">
    To integrate your LLM with xpander's tools and local functions, you'll need to select **one** LLM provider and model combination. Import the necessary enums from the SDK:

    <CodeGroup>
    ```python OpenAI
    from xpander_sdk import OpenAISupportedModels, LLMProvider
    ```

    ```python Amazon Bedrock
    from xpander_sdk import AmazonBedrockSupportedModels, LLMProvider
    ```

    ```python Nvidia NIM
    from xpander_sdk import NvidiaNIMSupportedModels, LLMProvider
    ```

    ```python Ollama
    from xpander_sdk import OllamaSupportedModels, LLMProvider
    ```


</CodeGroup>
  </Step>

  <Step title="Get Tools to the LLM">
    Configure your LLM client with the appropriate tools based on your provider:

    <CodeGroup>
    ```python OpenAI
    llm_response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=memory,
        tools=agent1.get_tools(llm_provider=LLMProvider.OPENAI)
    )
    ```

    ```python Amazon Bedrock
    llm_response = bedrock_client.invoke_model(
        modelId="anthropic.claude-3-haiku-20240307-v1:0",
        input={
            "messages": memory,
            "functions": agent1.get_tools(llm_provider=LLMProvider.AMAZON_BEDROCK)
        }
    )
    ```

    ```python Ollama
    llm_response = ollama_client.chat(
        model="qwen2.5-coder",
        messages=memory,
        tools=agent1.get_tools(llm_provider=LLMProvider.OLLAMA)
    )
    ```

    ```python Nvidia NIM
    llm_response = nvidia_nim_client.chat.completions.create(
        model="meta/llama-3.1-70b-instruct",
        messages=memory,
        tools=agent1.get_tools(llm_provider=LLMProvider.NVIDIA_NIM)
    )
    ```
    </CodeGroup>
  </Step>

  <Step title="Run Tools for the LLM">
    Process the LLM response and execute the tools:

    <CodeGroup>
    ```python OpenAI
    tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.OPENAI)
    tool_responses = agent1.run_tools(tools_to_run)
    ```

    ```python Amazon Bedrock
    tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.AMAZON_BEDROCK)
    tool_responses = agent1.run_tools(tools_to_run)
    ```

    ```python Ollama
    tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response, llm_provider=LLMProvider.OLLAMA)
    tool_responses = agent1.run_tools(tools_to_run)
    ```

    ```python Nvidia NIM
    tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.NVIDIA_NIM)
    tool_responses = agent1.run_tools(tools_to_run)
    ```
    </CodeGroup>
  </Step>

  <Step title="Understanding extract_tool_calls">
    The `extract_tool_calls` method is a crucial utility that handles the conversion between different LLM function/tool calling formats. Here's how it works:

    1. **Purpose**: It standardizes various LLM response formats into a unified format that xpander can process:
    ```python
    # Example of how extract_tool_calls processes different formats
    # OpenAI format
    openai_response = {
        "tool_calls": [
            {
                "function": {"name": "get_weather", "arguments": '{"location": "London"}'}
            }
        ]
    }

    # Bedrock/Claude format
    bedrock_response = {
        "function_calls": [
            {"name": "get_weather", "arguments": {"location": "London"}}
        ]
    }

    # Both get converted to a standardized internal format
    standardized_calls = [
        {
            "name": "get_weather",
            "arguments": {"location": "London"}
        }
    ]
    ```

    2. **Provider-Specific Handling**: The method automatically detects and processes different response structures:
    ```python
    # The llm_provider parameter tells extract_tool_calls how to parse the response
    tools_to_run = XpanderClient.extract_tool_calls(
        llm_response=llm_response,
        llm_provider=LLMProvider.OPENAI  # or AMAZON_BEDROCK, OLLAMA, NVIDIA_NIM
    )
    ```

    3. **Automatic API Translation**: Once standardized, these tool calls can be executed uniformly:
    ```python
    # The standardized format allows consistent execution across providers
    tool_responses = agent1.run_tools(tools_to_run)
    
    # Each tool_response contains the result of the API call
    # Example response:
    # {
    #     "name": "get_weather",
    #     "result": {"temperature": 20, "condition": "sunny"}
    # }
    ```
  </Step>
</Steps>

Need help? Visit our [Discord community](https://discord.gg/xpander) or [documentation](https://docs.xpander.ai). 
---
title: 'Bring Your Own LLM'
description: 'Guide to using xpander SDK with your own LLM deployment'
mode: 'wide'
---

While xpander.ai offers a serverless solution where you can use AI agents without managing LLM infrastructure, you might want to use your own LLM deployment for:

- Running models locally for lower latency
- Using specific model versions or custom fine-tuned models
- Implementing local function calling
- Maintaining full control over your LLM stack
- Handling sensitive data locally

This guide shows how to integrate your LLM with xpander's tools and local functions.

<Steps>
  <Step title="Prerequisites">
    1. Install Python 3.8 or higher
    2. xpander account and API key
    3. Your chosen LLM setup (OpenAI API or Ollama local deployment)
  </Step>

  <Step title="Installation">
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # On Windows: .venv\Scripts\activate
    pip install xpander-sdk openai ollama python-dotenv
    ```
  </Step>

  <Step title="Environment Setup">
    ```env
    XPANDER_API_KEY="your_xpander_key"
    XPANDER_AGENT_ID="your_agent_id"
    OPENAI_API_KEY="your_openai_key"  # If using OpenAI
    ```
  </Step>
</Steps>

# Choose Your LLM Provider

<CodeGroup>
```python OpenAI
from xpander_sdk import XpanderClient
from openai import OpenAI
import json

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
openai_client = OpenAI(api_key=openAIKey)

# Initialize conversation
memory = [
    {
        "role": "system",
        "content": "You are a helpful assistant with function calling and tool access. You are running in a while loop - add ##FINAL ANSWER## to stop."
    },
    {
        "role": "user",
        "content": "Get news about qwen2.5-coder"
    }
]
number_of_calls = 1

while True:
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    
    llm_response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=memory,
        tools=agent.get_tools()    
    )
    memory.append(llm_response.choices[0].message)

    if llm_response.choices[0].message.tool_calls:
        tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump())
        tool_responses = agent.run_tools(tools_to_run)
        
        for tool_response in tool_responses:
            memory.append({
                "role": "tool", 
                "content": json.dumps(tool_response.result), 
                "tool_call_id": tool_response.tool_call_id
            })

    if llm_response.choices[0].message.content:
        if "##FINAL ANSWER##" in llm_response.choices[0].message.content:
            break
            
    number_of_calls += 1

final_response = llm_response.choices[0].message.content
```

```python Ollama
from xpander_sdk import XpanderClient, LLMProvider
import ollama
import json

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
ollama_client = ollama.Client()

# Initialize conversation
memory = [
    {
        "role": "system",
        "content": "You are a helpful assistant with function calling and tool access. You are running in a while loop - add ##FINAL ANSWER## to stop."
    },
    {
        "role": "user",
        "content": "Get news about qwen2.5-coder from HackerNews"
    }
]

number_of_calls = 1

while True:
    llm_response = ollama_client.chat(
        model="qwen2.5-coder:14b",
        messages=memory,
        tools=agent.get_tools()    
    )
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    memory.append(llm_response['message'])

    if llm_response['message'].get('tool_calls'):
        tools_to_run = XpanderClient.extract_tool_calls(
            llm_response=llm_response, 
            llm_provider=LLMProvider.OLLAMA
        )
        tool_responses = agent.run_tools(tool_calls=tools_to_run)
        
        for tool_response in tool_responses:
            memory.append({
                "role": "tool",
                "content": tool_response.result,
                "tool_call_id": tool_response.tool_call_id
            })

    if llm_response['message'].get('content'):
        if "##FINAL ANSWER##" in llm_response['message']['content']:
            break

    number_of_calls += 1

final_response = llm_response['message']['content']
```
</CodeGroup>

# Best Practices

1. **Error Handling**: Implement robust error handling for both LLM and tool calls
2. **Memory Management**: Properly manage conversation context
3. **Security**: Validate all inputs and file operations
4. **Monitoring**: Log LLM and tool performance metrics
5. **Testing**: Test with different prompts and tool combinations

Need help? Visit our [Discord community](https://discord.gg/xpander) or [documentation](https://docs.xpander.ai). 
---
title: 'Bring Your Own LLM'
description: 'Guide to using xpander SDK with your own LLM deployment'
# mode: 'wide'
---

While xpander.ai offers a serverless solution where you can use AI agents without managing LLM infrastructure, you might want to use your own LLM deployment for:

- Running models locally for lower latency
- Using specific model versions or custom fine-tuned models
- Implementing local function calling
- Maintaining full control over your LLM stack
- Handling sensitive data locally

This guide shows how to integrate your LLM with xpander's tools and local functions.

# Choose Your LLM Provider

To integrate your LLM with xpander's tools and local functions, you'll need to select **one** LLM provider and model combination. The integration is done by configuring your agent object with the chosen provider and model.

## Provider and Model Selection

You can specify your chosen LLM provider and model using either enums from the SDK or string values:

```python
from xpander_sdk import (
    LLMProvider,  # Enum for supported LLM providers
    OpenAISupportedModels,  # Enum for Function calling LLMs
    NvidiaNIMSupportedModels,
    OllamaSupportedModels,
    AmazonBedrockSupportedModels
)
```

## Get Tools to the LLM

<CodeGroup>
```python OpenAI
llm_response = openai_client.chat.completions.create(
    model="gpt-4o",
    messages=memory,
    tools=agent1.get_tools(llm_provider=LLMProvider.OPENAI)
)
```

```python Amazon Bedrock
llm_response = bedrock_client.invoke_model(
    modelId="anthropic.claude-3-haiku-20240307-v1:0",
    input={
        "messages": memory,
        "functions": agent1.get_tools(llm_provider=LLMProvider.AMAZON_BEDROCK)
    }
)
```

```python Ollama
llm_response = ollama_client.chat(
    model="qwen2.5-coder",
    messages=memory,
    tools=agent1.get_tools(llm_provider=LLMProvider.OLLAMA)
)
```

```python Nvidia NIM
llm_response = nvidia_nim_client.chat.completions.create(
    model="meta/llama-3.1-70b-instruct",
    messages=memory,
    tools=agent1.get_tools(llm_provider=LLMProvider.NVIDIA_NIM)
)
```

</CodeGroup>

## Run Tools for the LLM

<CodeGroup>
```python OpenAI
tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.OPENAI)
tool_responses = agent1.run_tools(tools_to_run)
```

```python Amazon Bedrock
tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.AMAZON_BEDROCK)
tool_responses = agent1.run_tools(tools_to_run)
```

```python Ollama
tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response, llm_provider=LLMProvider.OLLAMA)
tool_responses = agent1.run_tools(tools_to_run)
```

```python Nvidia NIM
tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.NVIDIA_NIM)
tool_responses = agent1.run_tools(tools_to_run)
```
</CodeGroup>


## Available Providers and Models

Here are all the supported LLM providers and their corresponding models. Choose the combination that best fits your needs:

| Framework      | Model Name                                | Model Identifier (String)                  |
|----------------|-------------------------------------------|-------------------------------------------|
| **Amazon Bedrock** | Anthropocene Claude 3 Haiku               | anthropic.claude-3-haiku-20240307-v1:0    |
|                 | Anthropocene Claude 3.5 Sonnet            | anthropic.claude-3-5-sonnet-20240620-v1:0 |
|                 | Cohere Command R                          | cohere.command-r-v1:0                     |
|                 | Cohere Command R Plus                     | cohere.command-r-plus-v1:0                |
|                 | Meta Llama 3 1.8B Instruct                | meta.llama3-1-8b-instruct-v1:0            |
|                 | Meta Llama 3 1.70B Instruct               | meta.llama3-1-70b-instruct-v1:0           |
|                 | Meta Llama 3 1.405B Instruct              | meta.llama3-1-405b-instruct-v1:0          |
|                 | Mistral Large 2402                        | mistral.mistral-large-2402-v1:0           |
|                 | Mistral Large 2407                        | mistral.mistral-large-2407-v1:0           |
|                 | Mistral Small 2402                        | mistral.mistral-small-2402-v1:0           |
| **Nvidia NIM** | Meta Llama 3.1 70B Instruct               | meta/llama-3.1-70b-instruct               |
| **Ollama**     | Qwen2.5-Coder                             | qwen2.5-coder                             |
| **OpenAI**     | OpenAI GPT-4                              | gpt-4                                     |
|                 | OpenAI GPT-4o                             | gpt-4o                                    |
|                 | OpenAI GPT-4o Mini                        | gpt-4o-mini                               |


# Tool Response

The `tool_response` object represents the output from a tool invoked by an AI Agent. It contains key attributes that provide information about the tool invocation's execution and results. This data is used to build the AI Agent's memory and guide further interactions.

## Tool Response Object Structure

| Attribute                  | Type     | Description                                                                 |
|----------------------------|----------|-----------------------------------------------------------------------------|
| `function_name`            | `str`    | The name of the invoked function or tool.                                  |
| `status_code`              | `int`    | The HTTP status code returned by the tool or system.                       |
| `result`                   | `dict`   | The actual response or output from the tool.                               |
| `payload`                  | `dict`   | The data payload generated by the AI for the tool invocation.              |
| `tool_call_id`             | `str`    | A unique identifier for the specific tool invocation.                      |

# Full example
<CodeGroup>
```python OpenAI
from xpander_sdk import XpanderClient
from openai import OpenAI
import json

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
openai_client = OpenAI(api_key=openAIKey)

# Initialize conversation
memory = [
    {
        "role": "system",
        "content": "You are a helpful assistant with function calling and tool access. You are running in a while loop - add ##FINAL ANSWER## to stop."
    },
    {
        "role": "user",
        "content": "Get news about qwen2.5-coder"
    }
]
number_of_calls = 1

while True:
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    
    llm_response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=memory,
        tools=agent.get_tools()    
    )
    memory.append(llm_response.choices[0].message)

    if llm_response.choices[0].message.tool_calls:
        tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump())
        tool_responses = agent.run_tools(tools_to_run)
        
        for tool_response in tool_responses:
            memory.append({
                "role": "tool", 
                "content": json.dumps(tool_response.result), 
                "tool_call_id": tool_response.tool_call_id
            })

    if llm_response.choices[0].message.content:
        if "##FINAL ANSWER##" in llm_response.choices[0].message.content:
            break
            
    number_of_calls += 1

final_response = llm_response.choices[0].message.content
```

```python Ollama
from xpander_sdk import XpanderClient, LLMProvider
import ollama
import json

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
ollama_client = ollama.Client()

# Initialize conversation
memory = [
    {
        "role": "system",
        "content": "You are a helpful assistant with function calling and tool access. You are running in a while loop - add ##FINAL ANSWER## to stop."
    },
    {
        "role": "user",
        "content": "Get news about qwen2.5-coder from HackerNews"
    }
]

number_of_calls = 1

while True:
    llm_response = ollama_client.chat(
        model="qwen2.5-coder:14b",
        messages=memory,
        tools=agent.get_tools()    
    )
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    memory.append(llm_response['message'])

    if llm_response['message'].get('tool_calls'):
        tools_to_run = XpanderClient.extract_tool_calls(
            llm_response=llm_response, 
            llm_provider=LLMProvider.OLLAMA
        )
        tool_responses = agent.run_tools(tool_calls=tools_to_run)
        
        for tool_response in tool_responses:
            memory.append({
                "role": "tool",
                "content": tool_response.result,
                "tool_call_id": tool_response.tool_call_id
            })

    if llm_response['message'].get('content'):
        if "##FINAL ANSWER##" in llm_response['message']['content']:
            break

    number_of_calls += 1

final_response = llm_response['message']['content']
```
</CodeGroup>

# Best Practices

1. **Error Handling**: Implement robust error handling for both LLM and tool calls
2. **Memory Management**: Properly manage conversation context
3. **Security**: Validate all inputs and file operations
4. **Monitoring**: Log LLM and tool performance metrics
5. **Testing**: Test with different prompts and tool combinations

Need help? Visit our [Discord community](https://discord.gg/xpander) or [documentation](https://docs.xpander.ai). 
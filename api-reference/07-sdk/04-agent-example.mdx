---
title: "SDK Agent Example"
description: "Example implementation of an AI agent using the SDK"
icon: "robot"
---

<CodeGroup>
```python OpenAI
from xpander_sdk import XpanderClient
from openai import OpenAI
import json

from dotenv import load_dotenv
import os
load_dotenv()
xpanderAPIKey = os.environ.get("XPANDER_API_KEY","") ## Your XPANDER API Personal Key
xpanderAgentID = os.environ.get("XPANDER_AGENT_ID", "") ## Your Agent ID
openAIKey = os.environ.get("OPENAI_API_KEY", "") ## Your Agent ID

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)  # Get agent instance with tools
openai_client = OpenAI(api_key=openAIKey)

# Initialize conversation with system prompt and user query
memory = [
    {
        "role": "system",
        "content": "You are a helpful assistant with function calling and tool access. You are running in a while loop - add ##FINAL ANSWER## to stop."
    },
    {
        "role": "user",
        "content": "Get news about qwen2.5-coder"  # Example user query
    }
]

# Main conversation loop
number_of_calls = 1

while True:
    # Add step counter to track conversation progress
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    
    # Get LLM response with available tools
    llm_response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=memory,
        tools=agent.get_tools()    # Provide tools for function calling
    )
    memory.append(llm_response.choices[0].message)

    # Handle tool calls if the LLM wants to use tools
    if llm_response.choices[0].message.tool_calls:
        tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump())
        tool_responses = agent.run_tools(tools_to_run)
        
        # Add tool responses to conversation memory
        for tool_response in tool_responses:
            memory.append({
                "role": "tool", 
                "content": json.dumps(tool_response.result), 
                "tool_call_id": tool_response.tool_call_id
            })

    # Check for conversation end marker
    if llm_response.choices[0].message.content:
        if "##FINAL ANSWER##" in llm_response.choices[0].message.content:
            break
            
    number_of_calls += 1

final_response = llm_response.choices[0].message.content
```

```python Ollama
from xpander_sdk import XpanderClient, LLMProvider
import ollama
import json

from dotenv import load_dotenv
import os
load_dotenv()
xpanderAPIKey = os.environ.get("XPANDER_API_KEY","") ## Your XPANDER API Personal Key
xpanderAgentID = os.environ.get("XPANDER_AGENT_ID", "") ## Your Agent ID
openAIKey = os.environ.get("OPENAI_API_KEY", "") ## Your Agent ID

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)  # Get agent instance with tools
ollama_client = ollama.Client()

# Initialize conversation with system prompt and user query
memory = [
    {
        "role": "system",
        "content": "You are a helpful assistant with function calling and tool access. You are running in a while loop - add ##FINAL ANSWER## to stop."
    },
    {
        "role": "user",
        "content": "Get news about qwen2.5-coder from HackerNews"  # Example user query
    }
]

# Main conversation loop
number_of_calls = 1

while True:
    # Get LLM response using Ollama with available tools
    llm_response = ollama_client.chat(
        model="qwen2.5-coder:14b",
        messages=memory,
        tools=agent.get_tools()    # Provide tools for function calling
    )
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    memory.append(llm_response['message'])

    # Handle tool calls if the LLM wants to use tools
    if llm_response['message'].get('tool_calls'):
        tools_to_run = XpanderClient.extract_tool_calls(
            llm_response=llm_response, 
            llm_provider=LLMProvider.OLLAMA  # Specify Ollama as the LLM provider
        )
        tool_responses = agent.run_tools(tool_calls=tools_to_run)
        
        # Add tool responses to conversation memory
        for tool_response in tool_responses:
            memory.append({
                "role": "tool",
                "content": tool_response.result,
                "tool_call_id": tool_response.tool_call_id
            })

    # Check for conversation end marker
    if llm_response['message'].get('content'):
        if "##FINAL ANSWER##" in llm_response['message']['content']:
            break

    number_of_calls += 1

final_response = llm_response['message']['content']
```
</CodeGroup>


## Understanding the While Loop and Agent Progress

The while loop is a crucial component in the agent's execution model, enabling it to:

1. **Progress Through the Agent Graph**: The agent moves through different nodes in the graph system, executing tools and making decisions based on the conversation context.

2. **Maintain Conversation State**: Each iteration maintains the conversation history in the `memory` list, allowing the agent to reference previous interactions and tool results.

3. **Handle Multi-Step Tasks**: Complex tasks often require multiple steps - the loop enables the agent to:
   - Make function calls
   - Process results 
   - Plan next actions
   - Move to different graph nodes

4. **Control Flow Management**: The loop continues until the agent determines it has completed its task, signaled by including "##FINAL ANSWER##" in its response.

### Loop Components Explained:

- `number_of_calls`: Tracks iteration count for monitoring and debugging
- `llm_response`: Gets the LLM's next action/response based on conversation history
- `tool_calls`: Handles function calling when the agent needs to use tools
- `memory`: Maintains the full conversation context including tool results

The agent uses this loop structure to navigate through the Agent Graph System, ensuring systematic progression through complex workflows while maintaining context and state.


<Info>
  For more examples with different LLM providers (OpenAI, Anthropic, Ollama, etc.), check out our [Agents Hub Examples Repository](https://github.com/xpander-ai/xpander-agents-hub/tree/main/agents/examples/model-providers).
</Info>


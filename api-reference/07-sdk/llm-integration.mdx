---
title: "LLM Integration"
description: "Integrating different LLM providers with the Xpander SDK"
icon: "microchip"
---

# LLM Integration

The Xpander SDK is designed to work with multiple Large Language Model (LLM) providers. This guide explains how to integrate various LLM providers with your Xpander agents.

<Tabs>
  <Tab title="Overview">
    ## Supported LLM Providers

    The SDK supports the following LLM providers through the `LLMProvider` enum:

    | Provider         | Enum Value                     | Format                            |
    |------------------|--------------------------------|-----------------------------------|
    | OpenAI           | `LLMProvider.OPEN_AI`          | Standard OpenAI format            |
    | Claude/Anthropic | `LLMProvider.FRIENDLI_AI`      | Claude via FriendliAI format      |
    | Gemini           | `LLMProvider.GEMINI_OPEN_AI`   | Google Gemini with OpenAI format  |
    | Ollama           | `LLMProvider.OLLAMA`           | Ollama format for local models    |
    | LangChain        | `LLMProvider.LANG_CHAIN`       | LangChain format                  |
    | Real-time OpenAI | `LLMProvider.REAL_TIME_OPEN_AI` | Real-time OpenAI format          |
    | NVIDIA NIM       | `LLMProvider.NVIDIA_NIM`       | NVIDIA NIM format                 |
    | Amazon Bedrock   | `LLMProvider.AMAZON_BEDROCK`   | Amazon Bedrock format             |

    ## Related Resources

    <CardGroup cols={2}>
      <Card title="Agent" icon="robot" href="/api-reference/07-sdk/agent">
        Learn about the core Agent class
      </Card>
      <Card title="Memory Management" icon="brain" href="/api-reference/07-sdk/agent/memory">
        Managing agent memory
      </Card>
      <Card title="Agent Tools" icon="wrench" href="/api-reference/07-sdk/agent/tools">
        Working with agent tools
      </Card>
      <Card title="Tool Models" icon="box" href="/api-reference/07-sdk/models/tool-models">
        Understanding tool data models
      </Card>
    </CardGroup>
  </Tab>

  <Tab title="Integration Basics">
    ## Integration Basics

    Integrating an LLM provider involves three key steps:

    1. **Initialize memory** - Initialize the agent's memory with execution input and instructions
    2. **Get tools** formatted for your specific LLM provider - The SDK automatically converts tools to the format needed by each provider
    3. **Extract tool calls** from the LLM response - You must specify which provider format to use

    ### Provider Specification Requirements

    > **Important**: The Xpander SDK requires explicit provider specification when getting tools and extracting tool calls. Always specify the `llm_provider` parameter with the appropriate provider enum value.

    <Steps>
      <Step title="Initialize memory">
        ```python
        from xpander_sdk import LLMProvider
        
        # Initialize memory - provider specification is optional
        agent.memory.init_messages(
            input=agent.execution.input_message,
            instructions=agent.instructions
        )
        ```
      </Step>
      <Step title="Get tools formatted for your LLM provider">
        ```python
        # Get tools in the format required by your specific LLM provider
        # Always specify the provider to get the correct format
        openai_tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)
        claude_tools = agent.get_tools(llm_provider=LLMProvider.FRIENDLI_AI)
        gemini_tools = agent.get_tools(llm_provider=LLMProvider.GEMINI_OPEN_AI)
        ```
      </Step>
      <Step title="Extract tool calls with explicit provider">
        ```python
        # Always explicitly specify the provider when extracting tool calls
        # This must match the provider used for getting tools
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.OPEN_AI
        )
        ```
      </Step>
    </Steps>

    ## Generic Integration Pattern

    You can use this pattern to integrate any LLM provider:

    <CodeBlock title="Generic LLM Integration">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    from your_llm_client import YourLLMClient

    # 1. Initialize client and get agent
    xpander_client = XpanderClient(api_key="your-api-key")
    agent = xpander_client.agents.get(agent_id="agent-id")
    llm_client = YourLLMClient()

    # 2. Add task
    agent.add_task(input="Your task input")

    # 3. Initialize memory
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
        # No need to specify llm_provider - it's optional
    )

    # 4. Run execution loop
    while not agent.is_finished():
        # Get tools in your LLM's format - specify your provider here
        provider_format = LLMProvider.OPEN_AI  # Or closest match to your LLM's format
        tools = agent.get_tools(llm_provider=provider_format)
        
        # Convert messages if needed for your LLM
        formatted_messages = convert_messages_for_your_llm(agent.messages)
        
        # Call your LLM
        response = llm_client.generate_response(
            messages=formatted_messages,
            tools=tools
        )
        
        # Convert response to a standard format with choices[0].message structure
        standard_response = convert_to_standard_format(response)
        
        # Add response to agent memory
        agent.add_messages(messages=standard_response)
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=standard_response,
            llm_provider=provider_format
        )
        
        # Run tools
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)

    # 5. Get final result
    result = agent.retrieve_execution_result()
    print(result.result)

    # Helper functions for your specific LLM
    def convert_messages_for_your_llm(messages):
        # Convert messages to the format expected by your LLM
        # ...
        return formatted_messages

    def convert_to_standard_format(response):
        # Convert your LLM's response to a format with choices[0].message structure
        # ...
        return standard_response
    ```
    </CodeBlock>
  </Tab>

  <Tab title="OpenAI">
    ## OpenAI Integration

    <CodeBlock title="OpenAI Integration">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    from openai import OpenAI
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()
    XPANDER_API_KEY = os.environ["XPANDER_API_KEY"]
    OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]

    # Initialize clients
    xpander_client = XpanderClient(api_key=XPANDER_API_KEY)
    openai_client = OpenAI(api_key=OPENAI_API_KEY)

    # Get agent
    agent = xpander_client.agents.get(agent_id="agent-1234")

    # Add task and initialize memory
    agent.add_task(input="What are the latest developments in AI?")
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
        # llm_provider parameter is optional
    )

    # Run until completion
    while not agent.is_finished():
        # Get OpenAI-formatted tools
        tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)
        
        # Call OpenAI
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=agent.messages,
            tools=tools,
            tool_choice="auto",
            temperature=0.0
        )
        
        # Add response to memory
        agent.add_messages(messages=response.model_dump())
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.OPEN_AI
        )
        
        # Execute tools if any
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)
            print(f"Executed {len(results)} tools")

    # Get final result
    result = agent.retrieve_execution_result()
    print(result.result)
    ```
    </CodeBlock>
  </Tab>

  <Tab title="Claude">
    ## Claude (Anthropic) Integration

    <CodeBlock title="Claude Integration">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    import anthropic
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()
    XPANDER_API_KEY = os.environ["XPANDER_API_KEY"]
    ANTHROPIC_API_KEY = os.environ["ANTHROPIC_API_KEY"]

    # Initialize clients
    xpander_client = XpanderClient(api_key=XPANDER_API_KEY)
    claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

    # Get agent
    agent = xpander_client.agents.get(agent_id="agent-1234")

    # Add task and initialize memory
    agent.add_task(input="What are the latest developments in AI?")
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
        # llm_provider parameter is optional
    )

    # Run until completion
    while not agent.is_finished():
        # Get Claude-formatted tools
        tools = agent.get_tools(llm_provider=LLMProvider.FRIENDLI_AI)
        
        # Convert messages to Claude format
        claude_messages = []
        for msg in agent.messages:
            role = msg.get("role")
            content = msg.get("content", "")
            
            if role == "system":
                # Claude includes system message directly in the messages array
                claude_messages.append({"role": "system", "content": content})
            elif role == "user":
                claude_messages.append({"role": "user", "content": content})
            elif role == "assistant":
                claude_messages.append({"role": "assistant", "content": content})
        
        # Call Claude
        response = claude_client.messages.create(
            model="claude-3-opus-20240229",
            messages=claude_messages,
            tools=tools,
            temperature=0.0
        )
        
        # Add response to memory
        agent.add_messages(messages=response.model_dump())
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.FRIENDLI_AI
        )
        
        # Execute tools if any
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)
            print(f"Executed {len(results)} tools")

    # Get final result
    result = agent.retrieve_execution_result()
    print(result.result)
    ```
    </CodeBlock>
  </Tab>

  <Tab title="Gemini">
    ## Google Gemini Integration

    <CodeBlock title="Gemini Integration">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    import google.generativeai as genai
    from dotenv import load_dotenv
    import os
    import json

    # Load environment variables
    load_dotenv()
    XPANDER_API_KEY = os.environ["XPANDER_API_KEY"]
    GEMINI_API_KEY = os.environ["GEMINI_API_KEY"]

    # Initialize clients
    xpander_client = XpanderClient(api_key=XPANDER_API_KEY)
    genai.configure(api_key=GEMINI_API_KEY)

    # Get agent
    agent = xpander_client.agents.get(agent_id="agent-1234")

    # Add task and initialize memory
    agent.add_task(input="What are the latest developments in AI?")
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
        # llm_provider parameter is optional
    )

    # Helper function to convert messages for Gemini
    def convert_to_gemini_messages(messages):
        gemini_messages = []
        
        for msg in messages:
            role = msg.get("role")
            content = msg.get("content", "")
            
            if role == "system":
                # Gemini doesn't have a system role, so we add it as a user message
                gemini_messages.append({"role": "user", "parts": [{"text": f"System: {content}"}]})
            elif role == "user":
                gemini_messages.append({"role": "user", "parts": [{"text": content}]})
            elif role == "assistant":
                gemini_messages.append({"role": "model", "parts": [{"text": content}]})
        
        return gemini_messages

    # Run until completion
    while not agent.is_finished():
        # Get Gemini-formatted tools
        tools = agent.get_tools(llm_provider=LLMProvider.GEMINI_OPEN_AI)
        
        # Convert messages to Gemini format
        gemini_messages = convert_to_gemini_messages(agent.messages)
        
        # Initialize Gemini model
        model = genai.GenerativeModel(
            model_name="gemini-1.5-pro",
            tools=tools,
            generation_config={"temperature": 0.0}
        )
        
        # Call Gemini
        chat = model.start_chat(history=gemini_messages)
        response = chat.send_message(
            "Please help with the task and use tools if needed.",
            tools=tools
        )
        
        # Convert response to dict for standard processing
        response_dict = {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": response.text,
                    "tool_calls": response.tool_calls if hasattr(response, "tool_calls") else []
                }
            }]
        }
        
        # Add response to memory
        agent.add_messages(messages=response_dict)
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response_dict,
            llm_provider=LLMProvider.GEMINI_OPEN_AI
        )
        
        # Execute tools if any
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)
            print(f"Executed {len(results)} tools")

    # Get final result
    result = agent.retrieve_execution_result()
    print(result.result)
    ```
    </CodeBlock>
  </Tab>

  <Tab title="Ollama">
    ## Ollama Integration

    <CodeBlock title="Ollama Integration">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    import requests
    import json
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()
    XPANDER_API_KEY = os.environ["XPANDER_API_KEY"]
    OLLAMA_BASE_URL = "http://localhost:11434"  # Default Ollama URL

    # Initialize client
    xpander_client = XpanderClient(api_key=XPANDER_API_KEY)

    # Get agent
    agent = xpander_client.agents.get(agent_id="agent-1234")

    # Add task and initialize memory
    agent.add_task(input="What are the latest developments in AI?")
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
        # llm_provider parameter is optional
    )

    # Helper function to call Ollama API
    def call_ollama(messages, tools=None, model="llama3:latest"):
        url = f"{OLLAMA_BASE_URL}/api/chat"
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": 0.0
            }
        }
        
        # Add tools if available
        if tools:
            payload["tools"] = tools
        
        response = requests.post(url, json=payload)
        return response.json()

    # Run until completion
    while not agent.is_finished():
        # Get Ollama-formatted tools - must specify the provider here
        tools = agent.get_tools(llm_provider=LLMProvider.OLLAMA)
        
        # Call Ollama
        ollama_response = call_ollama(
            messages=agent.messages,
            tools=tools,
            model="llama3:latest"  # Use your preferred model
        )
        
        # Format response to standard structure
        response_dict = {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": ollama_response.get("message", {}).get("content", ""),
                    "tool_calls": ollama_response.get("message", {}).get("tool_calls", [])
                }
            }]
        }
        
        # Add response to memory
        agent.add_messages(messages=response_dict)
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response_dict,
            llm_provider=LLMProvider.OLLAMA
        )
        
        # Execute tools if any
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)
            print(f"Executed {len(results)} tools")

    # Get final result
    result = agent.retrieve_execution_result()
    print(result.result)
    ```
    </CodeBlock>
  </Tab>

  <Tab title="Best Practices">
    ## Best Practices

    ### Provider Specification

    Always specify the correct provider for all operations:

    ```python
    # For retrieving tools
    tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)

    # For extracting tool calls
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response.model_dump(),
        llm_provider=LLMProvider.OPEN_AI
    )
    ```

    ### Provider Consistency

    Use the same provider for both retrieving tools and extracting tool calls:

    ```python
    # Use consistent providers
    provider = LLMProvider.OPEN_AI

    # When getting tools
    tools = agent.get_tools(llm_provider=provider)

    # When extracting tool calls
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response.model_dump(),
        llm_provider=provider
    )
    ```

    ### Memory Initialization

    Memory initialization doesn't require provider specification:

    ```python
    # Provider specification is optional for memory initialization
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
    )
    ```

    ### Rate Limiting

    Implement rate limiting to avoid hitting API limits:

    ```python
    import time

    # Add a small delay between API calls
    time.sleep(1)
    ```

    ### Error Handling

    Implement proper error handling for LLM API calls and tool call extraction:

    ```python
    try:
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response,
            llm_provider=LLMProvider.OPEN_AI
        )
        
        # Handle case where no tool calls were found
        if not tool_calls and expected_tools:
            print("No tool calls found in response")
            # Handle the situation appropriately
            
    except Exception as e:
        print(f"Error extracting tool calls: {str(e)}")
        # Handle the error or use a default response
    ```

    ### Multi-Provider Support

    When working with multiple providers, always track which provider is being used:

    ```python
    # Define providers
    openai_provider = LLMProvider.OPEN_AI
    claude_provider = LLMProvider.FRIENDLI_AI

    # Function that includes provider tracking
    def process_with_provider(agent, response, provider):
        # Add messages to memory
        agent.add_messages(messages=response)
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response,
            llm_provider=provider
        )
        
        # Run tools
        return agent.run_tools(tool_calls=tool_calls)
    ```
  </Tab>

  <Tab title="Message Format">
    ## Message Format Handling

    The Xpander SDK automatically handles all message format conversions for all LLM providers:

    ### Internal Message Storage

    Xpander SDK stores all messages in a standard format in `agent.messages`. This format is compatible with OpenAI and OpenAI-compatible APIs out of the box.

    ```json
    [
      {
        "role": "system",
        "content": "System instructions..."
      },
      {
        "role": "user",
        "content": "User query..."
      },
      {
        "role": "assistant",
        "content": "Assistant response...",
        "tool_calls": [...]  // If any tools were called
      }
    ]
    ```

    ### Automatic Format Conversion

    When using any LLM provider, the SDK handles conversions automatically:

    1. For **all providers**, you can use `agent.messages` directly:

    ```python
    # For any provider (OpenAI, Claude, Gemini, etc.)
    # No manual conversion needed - just specify the right LLMProvider when getting tools

    # For OpenAI
    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=agent.messages,  # Use directly, no conversion needed
        tools=agent.get_tools(llm_provider=LLMProvider.OPEN_AI),
        temperature=0
    )

    # For Claude
    response = claude_client.messages.create(
        model="claude-3-opus-20240229",
        messages=agent.messages,  # Use directly, no conversion needed
        tools=agent.get_tools(llm_provider=LLMProvider.FRIENDLI_AI),
        temperature=0
    )
    ```

    2. For **adding responses back** to memory:

    ```python
    # For any provider, add response directly
    # The SDK automatically handles format detection and normalization
    agent.add_messages(messages=response)

    # For OpenAI client library which provides model_dump()
    agent.add_messages(messages=response.model_dump())
    ```

    3. When **extracting tool calls**, just specify the right provider:

    ```python
    # Extract tool calls by specifying the provider
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response,
        llm_provider=LLMProvider.FRIENDLI_AI  # Specify the provider
    )
    ```

    > **Important:** You don't need to manually convert messages between formats. The SDK automatically handles all conversions internally. 

    ### Provider Specification

    Always specify the appropriate provider when extracting tool calls:

    ```python
    # Explicitly specify the provider for extracting tool calls
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response.model_dump(),
        llm_provider=LLMProvider.OPEN_AI  # Must match the provider used for getting tools
    )
    ```

    For consistent behavior, use the same provider for both getting tools and extracting tool calls:

    ```python
    # Use the same provider in both places
    tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)
    # ...call LLM...
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response.model_dump(),
        llm_provider=LLMProvider.OPEN_AI  # Same provider as used for tools
    )
    ```
  </Tab>
</Tabs> 
---
title: 'SDK Function Calling'
description: 'How to use the xpander AI SDK to work with tools and function calling'
---

## Getting Tools

The xpander SDK allows you to retrieve tools connected to your agent in the xpander platform.

### Request

<CodeGroup>

```python Python
# Get tools from a specific agent
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
tools = agent.get_tools()
```

```javascript NodeJS
const agent = xpanderClient.agents.get(agentId);
const tools = agent.getTools();
```

```csharp .NET
var agent = xpanderClient.Agents.Get(agentId);
var tools = agent.GetTools();
```

```java Java
Agent agent = xpanderClient.agents.get(agentId);
Tool[] tools = agent.getTools();
```

</CodeGroup>

### Response

The tools are returned in the format required by your LLM provider. Here's an example response for different providers:

<CodeGroup>

```json OpenAI Format
[
  {
    "type": "function",
    "function": {
      "name": "search_articles",
      "description": "Search articles in the system with optional filters",
      "parameters": {
        "type": "object",
        "properties": {
          "query_params": {
            "type": "object",
            "properties": {
              "tag": { "type": "string" },
              "author": { "type": "string" }
            }
          },
          "path_params": {
            "type": "object",
            "properties": {}
          },
          "body_params": {
            "type": "object",
            "properties": {}
          }
        },
        "required": ["query_params", "path_params", "body_params"]
      }
    }
  }
]
```

</CodeGroup>

## Using Tools

To use tools with your LLM, follow these steps:

1. Get the tools from your agent
2. Pass them to your LLM
3. Extract and execute any tool calls
4. Process the results

### Example Implementation

<CodeGroup>

```python Python
# 1. Get tools from agent
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
tools = agent.get_tools()

# 2. Pass tools to LLM (OpenAI example)
llm_response = openai_client.chat.completions.create(
    model="gpt-4",
    messages=conversation_history,
    tools=tools    
)

# 3. Extract and execute tool calls
if llm_response.choices[0].message.tool_calls:
    # Extract tool calls
    tools_to_run = XpanderClient.extract_tool_calls(
        llm_response=llm_response.model_dump()
    )
    
    # Execute tools
    tool_responses = agent.run_tools(tools_to_run)
    
    # 4. Process results
    for tool_response in tool_responses:
        result = {
            "function_name": tool_response.function_name,
            "status_code": tool_response.status_code,
            "result": tool_response.result,
            "payload": tool_response.payload,
            "tool_call_id": tool_response.tool_call_id
        }
        # Add result to conversation history
        conversation_history.append({
            "role": "tool",
            "content": json.dumps(tool_response.result),
            "tool_call_id": tool_response.tool_call_id
        })
```

</CodeGroup>

### Tool Response Structure

When a tool is executed, it returns a response with the following properties:

- `function_name`: Name of the function that was called
- `status_code`: HTTP status code from the API call
- `result`: The actual response data from the tool
- `payload`: The parameters that were sent to the tool
- `tool_call_id`: Unique identifier for the tool call

## Best Practices

1. **Error Handling**: Always check the status code in tool responses to ensure the call was successful.

2. **Conversation Management**: Keep track of tool responses in your conversation history to give the LLM context about previous actions.

3. **Tool Selection**: Let the LLM choose which tools to use based on the user's request rather than pre-selecting tools.

4. **Validation**: Validate tool inputs before execution to prevent errors and improve reliability.

---
title: 'SDK Function Calling'
description: 'How to use the xpander AI SDK to work with tools and function calling'
---

## Getting Tools

The xpander SDK allows you to retrieve tools connected to your agent in the xpander platform.

### Request

<CodeGroup>

```python Python
# Get tools from a specific agent
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
tools = agent.get_tools()
```

```javascript NodeJS
const agent = xpanderClient.agents.get(agentId);
const tools = agent.getTools();
```

```csharp .NET
var agent = xpanderClient.Agents.Get(agentId);
var tools = agent.GetTools();
```

```java Java
Agent agent = xpanderClient.agents.get(agentId);
Tool[] tools = agent.getTools();
```

</CodeGroup>

### Response

The tools are returned in the format required by your LLM provider. Here's an example response for different providers:

<CodeGroup>

```json OpenAI Format
[
  {
    "type": "function",
    "function": {
      "name": "search_articles",
      "description": "Search articles in the system with optional filters",
      "parameters": {
        "type": "object",
        "properties": {
          "query_params": {
            "type": "object",
            "properties": {
              "tag": { "type": "string" },
              "author": { "type": "string" }
            }
          },
          "path_params": {
            "type": "object",
            "properties": {}
          },
          "body_params": {
            "type": "object",
            "properties": {}
          }
        },
        "required": ["query_params", "path_params", "body_params"]
      }
    }
  }
]
```

</CodeGroup>

## Run Tools

To use tools with your LLM, follow these steps:

1. Get the tools from your agent
2. Pass them to your LLM
3. Extract and execute any tool calls
4. Process the results

```python Passing tools to LLM
llm_response = openai_client.chat.completions.create(
    model="gpt-4",
    messages=conversation_history,
    tools=tools
) 
```

<Tip>
    You can use the `extract_tool_calls` method to extract tool calls from the LLM response or the `run_tools` method to execute them manually
</Tip>
### Function Calling Methods

There are two main approaches to executing tools. The SDK supports both parallel and sequential execution of tools

1. **Automated LLM Parsing** (Recommended):
```python
# Parse LLM response automatically into tool calls
tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump())

# Execute multiple tool calls at once
results = agent.run_tools(tools_to_run)  
```

2. **Manual Invocation**:
```python
from xpander_sdk import ToolCall, ToolCallType

# Create a ToolCall object
toolcall = ToolCall(
    name="tavily-insights-fetchInsightsFromTavilyAI",
    type=ToolCallType.XPANDER,
    payload={
        "bodyParams": {
            "query": "qwen2.5-coder news on HackerNews",
            "max_results": "5",
            "include_raw_content": "true",
            "include_images": "false",
            "exclude_domains": ["example.com"], 
        },
        "queryParams": {},
        "pathParams": {}
    }
)

# Execute single tool call
result = agent.run_tool(toolcall)
```

### Tool Classes

The SDK provides two main classes for working with tools:

#### ToolCall
A class representing a single tool invocation with the following attributes:
- `name`: (str) The identifier of the tool to be called
- `type`: (ToolCallType) The type of the tool call, usually ToolCallType.XPANDER
- `payload`: (dict) Contains three parameter dictionaries:
  - `bodyParams`: Parameters sent in the request body
  - `queryParams`: URL query parameters
  - `pathParams`: URL path parameters

#### ToolCallType
An enum class that defines the types of tool calls:
- `XPANDER`: Standard xpander.ai platform tools
- `FUNCTION`: Traditional function calls (For example local functions)

<Note>
The key differences are:

- `XpanderClient.extract_tool_calls()`: Static utility method that converts LLM-specific function call formats (OpenAI, Anthropic, etc.) into xpander's standardized ToolCall format
- `agent.run_tools()`: Executes one or more tool calls, accepting either:
  - Parsed ToolCall objects from extract_tool_calls()
  - Manual dictionary format for direct invocation
</Note>


## Tool Response

When a tool is executed (using `agent.run_tools()` or `agent.run_tool()`), it returns a response with the following properties:

- `function_name`: Name of the function that was called
- `status_code`: HTTP status code from the API call
- `result`: The actual response data from the tool
- `payload`: The parameters that were sent to the tool
- `tool_call_id`: Unique identifier for the tool call

Example:

```python
tool_response = agent.run_tool(toolcall)
print(tool_response.function_name) ## will print the name of the tool that was called
print(tool_response.status_code) ## will print the status code of the tool call (Http status code)
print(tool_response.result) ## will print the result of the tool call (JSON)
print(tool_response.payload) ## will print the payload sent to the tool (JSON)
print(tool_response.tool_call_id) ## will print the unique identifier for the tool call (Critical for LLMs)
```

# Full example
<CodeGroup>

```python Python
# 1. Get tools from agent
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
tools = agent.get_tools()

# 2. Pass tools to LLM (OpenAI example)
llm_response = openai_client.chat.completions.create(
    model="gpt-4",
    messages=conversation_history,
    tools=tools    
)

# 3. Extract and execute tool calls
if llm_response.choices[0].message.tool_calls:
    # Extract tool calls
    tools_to_run = XpanderClient.extract_tool_calls(
        llm_response=llm_response.model_dump()
    )
    
    # Execute tools
    tool_responses = agent.run_tools(tools_to_run)
    
    # 4. Process results
    for tool_response in tool_responses:
        result = {
            "function_name": tool_response.function_name,
            "status_code": tool_response.status_code,
            "result": tool_response.result,
            "payload": tool_response.payload,
            "tool_call_id": tool_response.tool_call_id
        }
        # Add result to conversation history
        conversation_history.append({
            "role": "tool",
            "content": json.dumps(tool_response.result),
            "tool_call_id": tool_response.tool_call_id
        })
```

</CodeGroup>


## Best Practices

1. **Error Handling**: Always check the status code in tool responses to ensure the call was successful.

2. **Conversation Management**: Keep track of tool responses in your conversation history to give the LLM context about previous actions and preseve the same tool call id for the same tool call.

3. **Tool Selection**: Let the LLM choose which tools to use based on the user's request rather than pre-selecting tools.

4. **Validation**: Validate tool inputs before execution to prevent errors and improve reliability.

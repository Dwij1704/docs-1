---
title: 'SDK Function Calling'
description: 'How to use the xpander AI SDK to work with tools and function calling'
---

## Getting Tools

The xpander SDK allows you to retrieve tools connected to your agent in the xpander platform.

### Request

<CodeGroup>

```python Python
# Get tools from a specific agent
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
tools = agent.get_tools()
```

```javascript NodeJS
const agent = xpanderClient.agents.get(agentId);
const tools = agent.getTools();
```

```csharp .NET
var agent = xpanderClient.Agents.Get(agentId);
var tools = agent.GetTools();
```

```java Java
Agent agent = xpanderClient.agents.get(agentId);
Tool[] tools = agent.getTools();
```

</CodeGroup>

### Response

The tools are returned in the format required by your LLM provider. Here's an example response for different providers:

<CodeGroup>

```json OpenAI Format
[
  {
    "type": "function",
    "function": {
      "name": "search_articles",
      "description": "Search articles in the system with optional filters",
      "parameters": {
        "type": "object",
        "properties": {
          "query_params": {
            "type": "object",
            "properties": {
              "tag": { "type": "string" },
              "author": { "type": "string" }
            }
          },
          "path_params": {
            "type": "object",
            "properties": {}
          },
          "body_params": {
            "type": "object",
            "properties": {}
          }
        },
        "required": ["query_params", "path_params", "body_params"]
      }
    }
  }
]
```

</CodeGroup>

## Run Tools

To use tools with your LLM, follow these steps:

1. Get the tools from your agent
2. Pass them to your LLM
3. Extract and execute any tool calls
4. Process the results

```python Passing tools to LLM
llm_response = openai_client.chat.completions.create(
    model="gpt-4",
    messages=conversation_history,
    tools=tools
) 
```

### Function Calling Methods

There are two main approaches to executing tools. The SDK supports both parallel and sequential execution of tools

1. **Automated LLM Parsing** (Recommended):
```python
# Parse LLM response automatically into tool calls
tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump())

# Execute multiple tool calls at once
results = agent.run_tools(tools_to_run)  
```

<Tip>
    The `extract_tool_calls` method to extract tool calls from the LLM response and the `run_tools` method to execute according to the rules of the agent.
</Tip>

2. **Manual Invocation**:
```python
from xpander_sdk import ToolCall, ToolCallType

# Create a ToolCall object
toolcall = ToolCall(
    name="tavily-insights-fetchInsightsFromTavilyAI",
    type=ToolCallType.XPANDER,
    payload={
        "bodyParams": {
            "query": "qwen2.5-coder news on HackerNews",
            "max_results": "5",
            "include_raw_content": "true",
            "include_images": "false",
            "exclude_domains": ["example.com"], 
        },
        "queryParams": {},
        "pathParams": {}
    }
)

# Execute single tool call
result = agent.run_tool(toolcall)
```


### Tool Classes

The SDK provides two main classes for working with tools:

#### ToolCall
A class representing a single tool invocation with the following attributes:
- `name`: (str) The identifier of the tool to be called
- `type`: (ToolCallType) The type of the tool call, usually ToolCallType.XPANDER
- `payload`: (dict) Contains three parameter dictionaries:
  - `bodyParams`: Parameters sent in the request body
  - `queryParams`: URL query parameters
  - `pathParams`: URL path parameters

#### ToolCallType
An enum class that defines the types of tool calls:
- `XPANDER`: Standard xpander.ai platform tools
- `FUNCTION`: Traditional function calls (For example local functions)

<Note>
The key differences are:

- `XpanderClient.extract_tool_calls()`: Static utility method that converts LLM-specific function call formats (OpenAI, Anthropic, etc.) into xpander's standardized ToolCall format
- `agent.run_tools()`: Executes one or more tool calls, accepting either:
  - Parsed ToolCall objects from extract_tool_calls()
  - Manual dictionary format for direct invocation
</Note>

# Tool Response

The `tool_response` object represents the output from a tool invoked by an AI Agent. It contains key attributes that provide information about the tool invocation's execution and results. This data is used to build the AI Agent's memory and guide further interactions.

## Tool Response Object Structure

| Attribute                  | Type     | Description                                                                 |
|----------------------------|----------|-----------------------------------------------------------------------------|
| `function_name`            | `str`    | The name of the invoked function or tool.                                  |
| `status_code`              | `int`    | The HTTP status code returned by the tool or system.                       |
| `result`                   | `dict`   | The actual response or output from the tool.                               |
| `payload`                  | `dict`   | The data payload generated by the AI for the tool invocation.              |
| `tool_call_id`             | `str`    | A unique identifier for the specific tool invocation.                      |

# Full example
<CodeGroup>
```python OpenAI
from xpander_sdk import XpanderClient
from openai import OpenAI
import json

from dotenv import load_dotenv
import os
load_dotenv()
xpanderAPIKey = os.environ.get("XPANDER_API_KEY","") ## Your XPANDER API Personal Key
xpanderAgentID = os.environ.get("XPANDER_AGENT_ID", "") ## Your Agent ID
openAIKey = os.environ.get("OPENAI_API_KEY", "") ## Your Agent ID

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
openai_client = OpenAI(api_key=openAIKey)

# Initialize conversation
memory = [
    {
        "role": "system",
        "content": "You are a helpful assistant with function calling and tool access. You are running in a while loop - add ##FINAL ANSWER## to stop."
    },
    {
        "role": "user",
        "content": "Get news about qwen2.5-coder"
    }
]
number_of_calls = 1

while True:
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    
    llm_response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=memory,
        tools=agent.get_tools()    
    )
    memory.append(llm_response.choices[0].message)

    if llm_response.choices[0].message.tool_calls:
        tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump())
        tool_responses = agent.run_tools(tools_to_run)
        
        for tool_response in tool_responses:
            memory.append({
                "role": "tool", 
                "content": json.dumps(tool_response.result), 
                "tool_call_id": tool_response.tool_call_id
            })

    if llm_response.choices[0].message.content:
        if "##FINAL ANSWER##" in llm_response.choices[0].message.content:
            break
            
    number_of_calls += 1

final_response = llm_response.choices[0].message.content
```

```python Ollama
from xpander_sdk import XpanderClient, LLMProvider
import ollama
import json

from dotenv import load_dotenv
import os
load_dotenv()
xpanderAPIKey = os.environ.get("XPANDER_API_KEY","") ## Your XPANDER API Personal Key
xpanderAgentID = os.environ.get("XPANDER_AGENT_ID", "") ## Your Agent ID
openAIKey = os.environ.get("OPENAI_API_KEY", "") ## Your Agent ID

# Initialize clients
xpander_client = XpanderClient(api_key=xpanderAPIKey)
agent = xpander_client.agents.get(agent_id=xpanderAgentID)
ollama_client = ollama.Client()

# Initialize conversation
memory = [
    {
        "role": "system",
        "content": "You are a helpful assistant with function calling and tool access. You are running in a while loop - add ##FINAL ANSWER## to stop."
    },
    {
        "role": "user",
        "content": "Get news about qwen2.5-coder from HackerNews"
    }
]

number_of_calls = 1

while True:
    llm_response = ollama_client.chat(
        model="qwen2.5-coder:14b",
        messages=memory,
        tools=agent.get_tools()    
    )
    memory.append({"role": "assistant", "content": f'Step number: {number_of_calls}'})
    memory.append(llm_response['message'])

    if llm_response['message'].get('tool_calls'):
        tools_to_run = XpanderClient.extract_tool_calls(
            llm_response=llm_response, 
            llm_provider=LLMProvider.OLLAMA
        )
        tool_responses = agent.run_tools(tool_calls=tools_to_run)
        
        for tool_response in tool_responses:
            memory.append({
                "role": "tool",
                "content": tool_response.result,
                "tool_call_id": tool_response.tool_call_id
            })

    if llm_response['message'].get('content'):
        if "##FINAL ANSWER##" in llm_response['message']['content']:
            break

    number_of_calls += 1

final_response = llm_response['message']['content']
```
</CodeGroup>


## Available Providers and Models

Here are all the supported LLM providers and their corresponding models. Choose the combination that best fits your needs:

| Framework      | Model Name                                | Model Identifier (String)                  |
|----------------|-------------------------------------------|-------------------------------------------|
| **Amazon Bedrock** | Anthropocene Claude 3 Haiku               | anthropic.claude-3-haiku-20240307-v1:0    |
|                 | Anthropocene Claude 3.5 Sonnet            | anthropic.claude-3-5-sonnet-20240620-v1:0 |
|                 | Cohere Command R                          | cohere.command-r-v1:0                     |
|                 | Cohere Command R Plus                     | cohere.command-r-plus-v1:0                |
|                 | Meta Llama 3 1.8B Instruct                | meta.llama3-1-8b-instruct-v1:0            |
|                 | Meta Llama 3 1.70B Instruct               | meta.llama3-1-70b-instruct-v1:0           |
|                 | Meta Llama 3 1.405B Instruct              | meta.llama3-1-405b-instruct-v1:0          |
|                 | Mistral Large 2402                        | mistral.mistral-large-2402-v1:0           |
|                 | Mistral Large 2407                        | mistral.mistral-large-2407-v1:0           |
|                 | Mistral Small 2402                        | mistral.mistral-small-2402-v1:0           |
| **Nvidia NIM** | Meta Llama 3.1 70B Instruct               | meta/llama-3.1-70b-instruct               |
| **Ollama**     | Qwen2.5-Coder                             | qwen2.5-coder                             |
| **OpenAI**     | OpenAI GPT-4                              | gpt-4                                     |
|                 | OpenAI GPT-4o                             | gpt-4o                                    |
|                 | OpenAI GPT-4o Mini                        | gpt-4o-mini                               |
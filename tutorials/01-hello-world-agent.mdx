---
title: "Building a Hello World Agent"
description: "Quick guide to programmatically build and run your first Xpander agent"
icon: "rocket"
---

# Hello World Agent Tutorial

This is a quick guide to create and run a simple agent using the Xpander SDK. The SDK is a powerful foundation of the platform (in fact, the UI itself uses the SDK). While the **Xpander UI provides a visual interface for creating agent graphs and visualizing tool relationships**, this guide provides programmers with a straightforward way to directly use the SDK.

## Step 1: Set up your environment

Create a new directory and install the required packages:

```bash
mkdir hello-agent
cd hello-agent
pip install xpander-sdk openai python-dotenv
```

Create a `.env` file with your API keys:

```
XPANDER_API_KEY=your_xpander_api_key
OPENAI_API_KEY=your_openai_api_key
```

## Step 2: Create a basic agent with reusable execution loop

Let's create an agent with a reusable execution loop function that includes monitoring capabilities. Create a file named `agent.py`:

```python
from xpander_sdk import XpanderClient, LLMProvider, Tokens, LLMTokens
from openai import OpenAI
import os
import time
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize clients
xpander_client = XpanderClient(api_key=os.environ['XPANDER_API_KEY'])
openai_client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

# Reusable agent execution loop with monitoring
def agent_loop(agent):
    """Run the agent's execution loop until completion with monitoring"""
    # Initialize token tracking and timing
    execution_tokens = Tokens(worker=LLMTokens(completion_tokens=0, prompt_tokens=0, total_tokens=0))
    execution_start_time = time.perf_counter()
    
    while not agent.is_finished():
        # Track start time for this inference
        start_time = time.perf_counter()
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=agent.messages,
            tools=agent.get_tools(llm_provider=LLMProvider.OPEN_AI),
            tool_choice=agent.tool_choice,
            temperature=0
        )
        
        # Track token usage
        execution_tokens.worker.completion_tokens += response.usage.completion_tokens
        execution_tokens.worker.prompt_tokens += response.usage.prompt_tokens
        execution_tokens.worker.total_tokens += response.usage.total_tokens
        
        # Report LLM usage to Xpander
        agent.report_llm_usage(
            llm_response=response.model_dump(),
            llm_inference_duration=time.perf_counter() - start_time,
            llm_provider=LLMProvider.OPEN_AI
        )
        
        agent.add_messages(response.model_dump())
        
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.OPEN_AI
        )
        
        if tool_calls:
            agent.run_tools(tool_calls=tool_calls)
    
    # Process results
    execution_end_time = time.perf_counter()
    
    # Report execution metrics to Xpander
    agent.report_execution_metrics(
        llm_tokens=execution_tokens,
        ai_model="gpt-4o"
    )
    
    print(f"Execution duration: {execution_end_time - execution_start_time:.2f} seconds")
    print(f"Total tokens used: {execution_tokens.worker.total_tokens}")

# Create a new agent
agent = xpander_client.agents.create(name="Hello World Agent")
agent.instructions.goal = "Answer questions helpfully and accurately."

# Save the agent ID for reuse - we'll sync later
agent_id = agent.id
print(f"Created agent ID: {agent_id}")
print(f"Add this to your .env: XPANDER_AGENT_ID={agent_id}")

# Sync the agent (with retry logic for better reliability)
retry_count = 0
max_retries = 3
success = False

while not success and retry_count < max_retries:
    try:
        print(f"Syncing agent (attempt {retry_count + 1})...")
        agent.sync()
        success = True
        print("Agent sync successful!")
    except Exception as e:
        retry_count += 1
        print(f"Sync error: {e}. Retrying in 2 seconds...")
        time.sleep(2)

if not success:
    print("Warning: Could not sync agent. Continuing without sync...")

# Ask the agent a question
agent.add_task(input="Hello! What can you do?")

# Run the agent execution loop
agent_loop(agent)

# Get the result
result = agent.retrieve_execution_result()
print(f"\nAgent response: {result.result}")
print(f"Thread ID: {result.memory_thread_id}")
print(f"\nYou can view this agent in the Xpander platform with ID: {agent_id}")
```

Run this script to create and test your agent:

```bash
python agent.py
```

## Step 3: Add thread management for conversations

Now let's enhance our agent with thread management for multi-turn conversations. Create a file named `chat.py`:

```python
from xpander_sdk import XpanderClient, LLMProvider, Tokens, LLMTokens
from openai import OpenAI
import os
import time
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize clients
xpander_client = XpanderClient(api_key=os.environ['XPANDER_API_KEY'])
openai_client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

# Reusable agent execution loop with monitoring
def agent_loop(agent):
    """Run the agent's execution loop until completion with monitoring"""
    # Initialize token tracking and timing
    execution_tokens = Tokens(worker=LLMTokens(completion_tokens=0, prompt_tokens=0, total_tokens=0))
    execution_start_time = time.perf_counter()
    
    while not agent.is_finished():
        # Track start time for this inference
        start_time = time.perf_counter()
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=agent.messages,
            tools=agent.get_tools(llm_provider=LLMProvider.OPEN_AI),
            tool_choice=agent.tool_choice,
            temperature=0
        )
        
        # Track token usage
        execution_tokens.worker.completion_tokens += response.usage.completion_tokens
        execution_tokens.worker.prompt_tokens += response.usage.prompt_tokens
        execution_tokens.worker.total_tokens += response.usage.total_tokens
        
        # Report LLM usage to Xpander
        agent.report_llm_usage(
            llm_response=response.model_dump(),
            llm_inference_duration=time.perf_counter() - start_time,
            llm_provider=LLMProvider.OPEN_AI
        )
        
        agent.add_messages(response.model_dump())
        
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.OPEN_AI
        )
        
        if tool_calls:
            agent.run_tools(tool_calls=tool_calls)
    
    # Process results
    execution_end_time = time.perf_counter()
    
    # Report execution metrics to Xpander
    agent.report_execution_metrics(
        llm_tokens=execution_tokens,
        ai_model="gpt-4o"
    )
    
    print(f"Execution duration: {execution_end_time - execution_start_time:.2f} seconds")
    print(f"Total tokens used: {execution_tokens.worker.total_tokens}")

# Function to chat with the agent
def chat(agent, message, thread_id=None):
    """Send a message to the agent and get a response"""
    print(f"\nUser: {message}")
    
    # Add task to agent (using thread_id for conversation continuity)
    agent.add_task(input=message, thread_id=thread_id)
    
    # Run the agent loop
    agent_loop(agent)
    
    # Get and return result
    result = agent.retrieve_execution_result()
    print(f"Agent: {result.result}")
    print(f"Thread ID: {result.memory_thread_id}")
    return result.memory_thread_id

# Load your existing agent
agent = xpander_client.agents.get(agent_id=os.environ['XPANDER_AGENT_ID'])
print(f"Loaded agent: {agent.name}")
print(f"You can view this agent in the Xpander platform with ID: {os.environ['XPANDER_AGENT_ID']}")

# Have a multi-turn conversation
thread_id = chat(agent, "What is machine learning?")
chat(agent, "Give me an example of a machine learning algorithm.", thread_id)
```

Make sure to add your agent ID to your `.env` file before running:

```bash
python chat.py
```

## Step 4: Add local tools

Let's enhance our agent with local tools. Create a file named `local_tools.py`:

```python
from xpander_sdk import XpanderClient, LLMProvider, ToolCallResult, Tokens, LLMTokens
from openai import OpenAI
import os
import time
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize clients
xpander_client = XpanderClient(api_key=os.environ['XPANDER_API_KEY'])
openai_client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

# Define local function
def get_current_time(timezone=None):
    """Get the current date and time with millisecond precision."""
    now = datetime.now()
    return f"Local Function Current time{' in '+timezone if timezone else ''}: {now}"

# Set up local tools
local_tools = [
    {
        "declaration": {
            "type": "function",
            "function": {
                "name": "get_current_time",
                "description": "Get the precise current date and time.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "timezone": {
                            "type": "string",
                            "description": "Timezone (e.g., 'UTC', 'EST')"
                        }
                    },
                    "required": []
                }
            }
        },
        "fn": get_current_time
    }
]

local_tools_list = [tool['declaration'] for tool in local_tools]
local_tools_by_name = {tool['declaration']['function']['name']: tool['fn'] 
                      for tool in local_tools}

# Enhanced agent execution loop with local tools and monitoring
def agent_loop(agent, local_tools_by_name=None):
    """Run the agent's execution loop until completion with monitoring and local tools"""
    # Initialize token tracking and timing
    execution_tokens = Tokens(worker=LLMTokens(completion_tokens=0, prompt_tokens=0, total_tokens=0))
    execution_start_time = time.perf_counter()
    
    while not agent.is_finished():
        # Track start time for this inference
        start_time = time.perf_counter()
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=agent.messages,
            tools=agent.get_tools(llm_provider=LLMProvider.OPEN_AI),
            tool_choice=agent.tool_choice,
            temperature=0
        )
        
        # Track token usage
        execution_tokens.worker.completion_tokens += response.usage.completion_tokens
        execution_tokens.worker.prompt_tokens += response.usage.prompt_tokens
        execution_tokens.worker.total_tokens += response.usage.total_tokens
        
        # Report LLM usage to Xpander
        agent.report_llm_usage(
            llm_response=response.model_dump(),
            llm_inference_duration=time.perf_counter() - start_time,
            llm_provider=LLMProvider.OPEN_AI
        )
        
        agent.add_messages(response.model_dump())
        
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.OPEN_AI
        )
        
        if tool_calls:
            # Run cloud tools
            agent.run_tools(tool_calls=tool_calls)
            
            # Run local tools if provided
            if local_tools_by_name:
                pending_local_tool_execution = XpanderClient.retrieve_pending_local_tool_calls(tool_calls=tool_calls)
                local_tools_results = []
                
                for tc in pending_local_tool_execution:
                    print(f"Running local tool: {tc.name}")
                    tool_call_result = ToolCallResult(function_name=tc.name, tool_call_id=tc.tool_call_id, payload=tc.payload)
                    try:
                        if tc.name in local_tools_by_name:
                            tool_call_result.is_success = True
                            tool_call_result.result = local_tools_by_name[tc.name](**tc.payload)
                        else:
                            raise Exception(f"Local tool {tc.name} not found")
                    except Exception as e:
                        tool_call_result.is_success = False
                        tool_call_result.is_error = True
                        tool_call_result.result = str(e)
                    finally:
                        local_tools_results.append(tool_call_result)

                if local_tools_results:
                    print(f"Registering {len(local_tools_results)} local tool results...")
                    agent.memory.add_tool_call_results(tool_call_results=local_tools_results)
    
    # Process results
    execution_end_time = time.perf_counter()
    
    # Report execution metrics to Xpander
    agent.report_execution_metrics(
        llm_tokens=execution_tokens,
        ai_model="gpt-4o"
    )
    
    print(f"Execution duration: {execution_end_time - execution_start_time:.2f} seconds")
    print(f"Total tokens used: {execution_tokens.worker.total_tokens}")

# Function to chat with the agent
def chat(agent, message, thread_id=None, local_tools_by_name=None):
    """Send a message to the agent and get a response"""
    print(f"\nUser: {message}")
    
    # Add task to agent (using thread_id for conversation continuity)
    agent.add_task(input=message, thread_id=thread_id)
    
    # Run the agent loop
    agent_loop(agent, local_tools_by_name)
    
    # Get and return result
    result = agent.retrieve_execution_result()
    print(f"Agent: {result.result}")
    print(f"Thread ID: {result.memory_thread_id}")
    return result.memory_thread_id

# Load your existing agent
agent = xpander_client.agents.get(agent_id=os.environ['XPANDER_AGENT_ID'])
print(f"Loaded agent: {agent.name}")
print(f"You can view this agent in the Xpander platform with ID: {os.environ['XPANDER_AGENT_ID']}")

# Add local tools to the agent
agent.add_local_tools(local_tools_list)
print(f"Local tool 'get_current_time' is ready to be used.")

# Test the time tool
thread_id = chat(agent, "What time is it right now? Use the get_current_time tool.", 
                local_tools_by_name=local_tools_by_name)
chat(agent, "Tell me more about time zones.", thread_id, 
    local_tools_by_name=local_tools_by_name)
```

Run your agent with local tools:

```bash
python local_tools.py
```

## Step 5: Add search capability

Now let's add Tavily search capability to our agent. Create a file named `search_agent.py`:

```python
from xpander_sdk import XpanderClient, LLMProvider, ToolCallResult, GraphItem, Tokens, LLMTokens
from openai import OpenAI
import os
import time
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize clients
xpander_client = XpanderClient(api_key=os.environ['XPANDER_API_KEY'])
openai_client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

# Define local function
def get_current_time(timezone=None):
    """Get the current date and time with millisecond precision."""
    now = datetime.now()
    return f"Local Function Current time{' in '+timezone if timezone else ''}: {now}"

# Set up local tools
local_tools = [
    {
        "declaration": {
            "type": "function",
            "function": {
                "name": "get_current_time",
                "description": "Get the precise current date and time.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "timezone": {
                            "type": "string",
                            "description": "Timezone (e.g., 'UTC', 'EST')"
                        }
                    },
                    "required": []
                }
            }
        },
        "fn": get_current_time
    }
]

local_tools_list = [tool['declaration'] for tool in local_tools]
local_tools_by_name = {tool['declaration']['function']['name']: tool['fn'] 
                      for tool in local_tools}

# Enhanced agent execution loop with local tools and monitoring
def agent_loop(agent, local_tools_by_name=None):
    """Run the agent's execution loop until completion with monitoring and local tools"""
    # Initialize token tracking and timing
    execution_tokens = Tokens(worker=LLMTokens(completion_tokens=0, prompt_tokens=0, total_tokens=0))
    execution_start_time = time.perf_counter()
    
    while not agent.is_finished():
        # Track start time for this inference
        start_time = time.perf_counter()
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=agent.messages,
            tools=agent.get_tools(llm_provider=LLMProvider.OPEN_AI),
            tool_choice=agent.tool_choice,
            temperature=0
        )
        
        # Track token usage
        execution_tokens.worker.completion_tokens += response.usage.completion_tokens
        execution_tokens.worker.prompt_tokens += response.usage.prompt_tokens
        execution_tokens.worker.total_tokens += response.usage.total_tokens
        
        # Report LLM usage to Xpander
        agent.report_llm_usage(
            llm_response=response.model_dump(),
            llm_inference_duration=time.perf_counter() - start_time,
            llm_provider=LLMProvider.OPEN_AI
        )
        
        agent.add_messages(response.model_dump())
        
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.OPEN_AI
        )
        
        if tool_calls:
            # Display which tools are being used
            for call in tool_calls:
                name = getattr(call, 'name', None) or getattr(getattr(call, 'function', {}), 'name', "unnamed")
                print(f"Using tool: {name}")
                
            # Run cloud tools
            agent.run_tools(tool_calls=tool_calls)
            
            # Run local tools if provided
            if local_tools_by_name:
                pending_local_tool_execution = XpanderClient.retrieve_pending_local_tool_calls(tool_calls=tool_calls)
                local_tools_results = []
                
                for tc in pending_local_tool_execution:
                    print(f"Running local tool: {tc.name}")
                    tool_call_result = ToolCallResult(function_name=tc.name, tool_call_id=tc.tool_call_id, payload=tc.payload)
                    try:
                        if tc.name in local_tools_by_name:
                            tool_call_result.is_success = True
                            tool_call_result.result = local_tools_by_name[tc.name](**tc.payload)
                        else:
                            raise Exception(f"Local tool {tc.name} not found")
                    except Exception as e:
                        tool_call_result.is_success = False
                        tool_call_result.is_error = True
                        tool_call_result.result = str(e)
                    finally:
                        local_tools_results.append(tool_call_result)

                if local_tools_results:
                    print(f"Registering {len(local_tools_results)} local tool results...")
                    agent.memory.add_tool_call_results(tool_call_results=local_tools_results)
    
    # Process results
    execution_end_time = time.perf_counter()
    
    # Report execution metrics to Xpander
    agent.report_execution_metrics(
        llm_tokens=execution_tokens,
        ai_model="gpt-4o"
    )
    
    print(f"Execution duration: {execution_end_time - execution_start_time:.2f} seconds")
    print(f"Total tokens used: {execution_tokens.worker.total_tokens}")

# Function to chat with the agent
def chat(agent, message, thread_id=None, local_tools_by_name=None):
    """Send a message to the agent and get a response"""
    print(f"\nUser: {message}")
    
    # Add task to agent (using thread_id for conversation continuity)
    agent.add_task(input=message, thread_id=thread_id)
    
    # Run the agent loop
    agent_loop(agent, local_tools_by_name)
    
    # Get and return result
    result = agent.retrieve_execution_result()
    print(f"Agent: {result.result}")
    print(f"Thread ID: {result.memory_thread_id}")
    return result.memory_thread_id

# Function to add Tavily search capability
def add_tavily_search(agent):
    """Add Tavily search capability to the agent"""
    # Check if Tavily search is already available
    print("Checking for existing search tools...")
    tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)
    for tool in tools:
        if 'function' in tool and tool['function']['name'] == 'XpanderNewsInsightsFetchTavilyAIInsights':
            print(f"✅ Tavily search already available as: {tool['function']['name']}")
            return True
    
    print("Adding Tavily search capability...")
    
    try:
        # Find the xpanderAI Tools interface with retry logic
        retry_attempts = 0
        max_retries = 3
        xpander_tools = None
        
        while retry_attempts < max_retries and not xpander_tools:
            try:
                available_interfaces = agent.retrieve_agentic_interfaces()
                xpander_tools = next((interface for interface in available_interfaces 
                              if "xpanderai tools" in interface.name.lower()), None)
                if not xpander_tools:
                    print(f"xpanderAI Tools interface not found. Retrying ({retry_attempts + 1}/{max_retries})...")
                    retry_attempts += 1
                    time.sleep(2)
            except Exception as e:
                print(f"Error retrieving interfaces: {e}. Retrying ({retry_attempts + 1}/{max_retries})...")
                retry_attempts += 1
                time.sleep(2)
        
        if not xpander_tools:
            print("❌ xpanderAI Tools interface not found after retries")
            return False
        
        # Find the Tavily operation with retry logic
        retry_attempts = 0
        tavily_op = None
        
        while retry_attempts < max_retries and not tavily_op:
            try:
                operations = agent.retrieve_agentic_operations(agentic_interface=xpander_tools)
                tavily_op = next((op for op in operations 
                              if "tavily" in op.name.lower()), None)
                if not tavily_op:
                    print(f"Tavily operation not found. Retrying ({retry_attempts + 1}/{max_retries})...")
                    retry_attempts += 1
                    time.sleep(2)
            except Exception as e:
                print(f"Error retrieving operations: {e}. Retrying ({retry_attempts + 1}/{max_retries})...")
                retry_attempts += 1
                time.sleep(2)
        
        if not tavily_op:
            print("❌ Tavily search operation not found after retries")
            return False
        
        print(f"Found Tavily operation: {tavily_op.name}")
        
        # Add the operation to the agent with retry logic
        print("Attaching search operation to agent...")
        retry_attempts = 0
        success = False
        
        while retry_attempts < max_retries and not success:
            try:
                agent.attach_operations(operations=[tavily_op])
                agent.sync()
                success = True
            except Exception as e:
                print(f"Error attaching operation: {e}. Retrying ({retry_attempts + 1}/{max_retries})...")
                retry_attempts += 1
                time.sleep(2)
        
        if not success:
            print("❌ Failed to attach search operation after retries")
            return False
        
        # Create a graph node for the operation with retry logic
        print("Creating graph node...")
        retry_attempts = 0
        success = False
        
        while retry_attempts < max_retries and not success:
            try:
                node = agent.graph.add_node(
                    GraphItem(
                        agent=agent,
                        item_id=tavily_op.id_to_use_on_graph,
                        name=tavily_op.name
                    )
                )
                agent.sync()
                success = True
            except Exception as e:
                print(f"Error creating graph node: {e}. Retrying ({retry_attempts + 1}/{max_retries})...")
                retry_attempts += 1
                time.sleep(2)
        
        if not success:
            print("❌ Failed to create graph node after retries")
            return False
        
        print("✅ Tavily search capability added successfully!")
        
        # Wait briefly for the search capability to be fully registered
        print("Waiting for search capability to be fully registered...")
        time.sleep(3)
        
        return True
    except Exception as e:
        print(f"Error setting up search: {e}")
        return False

# Load your existing agent
agent = xpander_client.agents.get(agent_id=os.environ['XPANDER_AGENT_ID'])
print(f"Loaded agent: {agent.name}")
print(f"You can view this agent in the Xpander platform with ID: {os.environ['XPANDER_AGENT_ID']}")

# Add local tools to the agent
agent.add_local_tools(local_tools_list)
print("Local tools added to agent")

# Add Tavily search capability
tavily_ready = add_tavily_search(agent)

# Test the agent with available capabilities
if tavily_ready:
    print("\nTesting agent with both time function and search capability...")
    # First message - asking about time
    thread_id = chat(agent, "What time is it right now? Use the get_current_time tool.", 
                    local_tools_by_name=local_tools_by_name)
    
    # Second message in same thread - asking about search
    chat(agent, "What are the latest developments in AI?", thread_id, 
        local_tools_by_name=local_tools_by_name)
else:
    print("\nTesting agent with time function only (search wasn't added successfully)...")
    # Just test the time function since search isn't ready yet
    thread_id = chat(agent, "What time is it right now? Use the get_current_time tool.", 
                    local_tools_by_name=local_tools_by_name)
```

Run the script to test your agent with search capability:

```bash
python search_agent.py
```

## Step 6: Complete agent with everything

Now that we've built each component separately, let's put everything together in a single file that provides all capabilities. Create a file named `complete_agent.py`:

```python
from xpander_sdk import XpanderClient, LLMProvider, ToolCallResult, GraphItem, Tokens, LLMTokens
from openai import OpenAI
import os
import time
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize clients
xpander_client = XpanderClient(api_key=os.environ['XPANDER_API_KEY'])
openai_client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

# Define local function
def get_current_time(timezone=None):
    """Get the current date and time with millisecond precision."""
    now = datetime.now()
    return f"Local Function Current time{' in '+timezone if timezone else ''}: {now}"

# Set up local tools
local_tools = [
    {
        "declaration": {
            "type": "function",
            "function": {
                "name": "get_current_time",
                "description": "Get the precise current date and time.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "timezone": {
                            "type": "string",
                            "description": "Timezone (e.g., 'UTC', 'EST')"
                        }
                    },
                    "required": []
                }
            }
        },
        "fn": get_current_time
    }
]

local_tools_list = [tool['declaration'] for tool in local_tools]
local_tools_by_name = {tool['declaration']['function']['name']: tool['fn'] 
                       for tool in local_tools}

# Reusable agent execution loop with monitoring
def agent_loop(agent, local_tools_by_name=None):
    """Run the agent's execution loop until completion with monitoring"""
    # Initialize token tracking and timing
    execution_tokens = Tokens(worker=LLMTokens(completion_tokens=0, prompt_tokens=0, total_tokens=0))
    execution_start_time = time.perf_counter()
    
    while not agent.is_finished():
        # Track start time for this inference
        start_time = time.perf_counter()
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=agent.messages,
            tools=agent.get_tools(llm_provider=LLMProvider.OPEN_AI),
            tool_choice=agent.tool_choice,
            temperature=0
        )
        
        # Track token usage
        execution_tokens.worker.completion_tokens += response.usage.completion_tokens
        execution_tokens.worker.prompt_tokens += response.usage.prompt_tokens
        execution_tokens.worker.total_tokens += response.usage.total_tokens
        
        # Report LLM usage to Xpander
        agent.report_llm_usage(
            llm_response=response.model_dump(),
            llm_inference_duration=time.perf_counter() - start_time,
            llm_provider=LLMProvider.OPEN_AI
        )
        
        agent.add_messages(response.model_dump())
        
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.OPEN_AI
        )
        
        if tool_calls:
            # Display which tools are being used
            for call in tool_calls:
                name = getattr(call, 'name', None) or getattr(getattr(call, 'function', {}), 'name', "unnamed")
                print(f"Using tool: {name}")
                
            # Run cloud tools
            agent.run_tools(tool_calls=tool_calls)
            
            # Run local tools if provided
            if local_tools_by_name:
                pending_local_tool_execution = XpanderClient.retrieve_pending_local_tool_calls(tool_calls=tool_calls)
                local_tools_results = []
                
                for tc in pending_local_tool_execution:
                    print(f"Running local tool: {tc.name}")
                    tool_call_result = ToolCallResult(function_name=tc.name, tool_call_id=tc.tool_call_id, payload=tc.payload)
                    try:
                        if tc.name in local_tools_by_name:
                            tool_call_result.is_success = True
                            tool_call_result.result = local_tools_by_name[tc.name](**tc.payload)
                        else:
                            raise Exception(f"Local tool {tc.name} not found")
                    except Exception as e:
                        tool_call_result.is_success = False
                        tool_call_result.is_error = True
                        tool_call_result.result = str(e)
                    finally:
                        local_tools_results.append(tool_call_result)

                if local_tools_results:
                    print(f"Registering {len(local_tools_results)} local tool results...")
                    agent.memory.add_tool_call_results(tool_call_results=local_tools_results)
    
    # Process results
    execution_end_time = time.perf_counter()
    
    # Report execution metrics to Xpander
    agent.report_execution_metrics(
        llm_tokens=execution_tokens,
        ai_model="gpt-4o"
    )
    
    print(f"Execution duration: {execution_end_time - execution_start_time:.2f} seconds")
    print(f"Total tokens used: {execution_tokens.worker.total_tokens}")

# Function to chat with the agent
def chat(agent, message, thread_id=None, local_tools_by_name=None):
    """Send a message to the agent and get a response"""
    print(f"\nUser: {message}")
    
    # Add task to agent (using thread_id for conversation continuity)
    agent.add_task(input=message, thread_id=thread_id)
    
    # Run the agent loop
    agent_loop(agent, local_tools_by_name)
    
    # Get and return result
    result = agent.retrieve_execution_result()
    print(f"Agent: {result.result}")
    print(f"Thread ID: {result.memory_thread_id}")
    return result.memory_thread_id

# Function to create or load an agent
def setup_agent():
    """Create a new agent or load an existing one"""
    if 'XPANDER_AGENT_ID' in os.environ and os.environ['XPANDER_AGENT_ID']:
        # Load existing agent
        agent = xpander_client.agents.get(agent_id=os.environ['XPANDER_AGENT_ID'])
        print(f"Loaded agent: {agent.name}")
    else:
        # Create a new agent
        agent = xpander_client.agents.create(name="Hello World Agent")
        agent.instructions.goal = "Answer questions helpfully and accurately."
        
        # Save the agent ID
        agent_id = agent.id
        print(f"Created agent ID: {agent_id}")
        print(f"Add this to your .env: XPANDER_AGENT_ID={agent_id}")
        
        # Sync the agent (with retry logic for better reliability)
        retry_count = 0
        max_retries = 3
        success = False
        
        while not success and retry_count < max_retries:
            try:
                print(f"Syncing agent (attempt {retry_count + 1})...")
                agent.sync()
                success = True
                print("Agent sync successful!")
            except Exception as e:
                retry_count += 1
                print(f"Sync error: {e}. Retrying in 2 seconds...")
                time.sleep(2)
        
        if not success:
            print("Warning: Could not sync agent. Continuing without sync...")
    
    print(f"You can view this agent in the Xpander platform with ID: {agent.id}")
    return agent

# Function to add Tavily search capability
def add_tavily_search(agent):
    """Add Tavily search capability to the agent"""
    # Check if Tavily search is already available
    tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)
    for tool in tools:
        if 'function' in tool and tool['function']['name'] == 'XpanderNewsInsightsFetchTavilyAIInsights':
            print(f"✅ Tavily search already available as: {tool['function']['name']}")
            return True
    
    print("Adding Tavily search capability...")
    
    try:
        # Find the xpanderAI Tools interface with retry logic
        retry_attempts = 0
        max_retries = 3
        xpander_tools = None
        
        while retry_attempts < max_retries and not xpander_tools:
            try:
                available_interfaces = agent.retrieve_agentic_interfaces()
                xpander_tools = next((interface for interface in available_interfaces 
                              if "xpanderai tools" in interface.name.lower()), None)
                if not xpander_tools:
                    print(f"xpanderAI Tools interface not found. Retrying ({retry_attempts + 1}/{max_retries})...")
                    retry_attempts += 1
                    time.sleep(2)
            except Exception as e:
                print(f"Error retrieving interfaces: {e}. Retrying ({retry_attempts + 1}/{max_retries})...")
                retry_attempts += 1
                time.sleep(2)
        
        if not xpander_tools:
            print("❌ xpanderAI Tools interface not found after retries")
            return False
        
        # Find the Tavily operation with retry logic
        retry_attempts = 0
        tavily_op = None
        
        while retry_attempts < max_retries and not tavily_op:
            try:
                operations = agent.retrieve_agentic_operations(agentic_interface=xpander_tools)
                tavily_op = next((op for op in operations 
                              if "tavily" in op.name.lower()), None)
                if not tavily_op:
                    print(f"Tavily operation not found. Retrying ({retry_attempts + 1}/{max_retries})...")
                    retry_attempts += 1
                    time.sleep(2)
            except Exception as e:
                print(f"Error retrieving operations: {e}. Retrying ({retry_attempts + 1}/{max_retries})...")
                retry_attempts += 1
                time.sleep(2)
        
        if not tavily_op:
            print("❌ Tavily search operation not found after retries")
            return False
        
        print(f"Found Tavily operation: {tavily_op.name}")
        
        # Add the operation to the agent with retry logic
        print("Attaching search operation to agent...")
        retry_attempts = 0
        success = False
        
        while retry_attempts < max_retries and not success:
            try:
                agent.attach_operations(operations=[tavily_op])
                agent.sync()
                success = True
            except Exception as e:
                print(f"Error attaching operation: {e}. Retrying ({retry_attempts + 1}/{max_retries})...")
                retry_attempts += 1
                time.sleep(2)
        
        if not success:
            print("❌ Failed to attach search operation after retries")
            return False
        
        # Create a graph node for the operation with retry logic
        print("Creating graph node...")
        retry_attempts = 0
        success = False
        
        while retry_attempts < max_retries and not success:
            try:
                node = agent.graph.add_node(
                    GraphItem(
                        agent=agent,
                        item_id=tavily_op.id_to_use_on_graph,
                        name=tavily_op.name
                    )
                )
                agent.sync()
                success = True
            except Exception as e:
                print(f"Error creating graph node: {e}. Retrying ({retry_attempts + 1}/{max_retries})...")
                retry_attempts += 1
                time.sleep(2)
        
        if not success:
            print("❌ Failed to create graph node after retries")
            return False
        
        print("✅ Tavily search capability added successfully!")
        
        # Wait briefly for the search capability to be fully registered
        print("Waiting for search capability to be fully registered...")
        time.sleep(3)
        
        return True
    except Exception as e:
        print(f"Error setting up search: {e}")
        return False

# Main execution - demonstrates different features
def main():
    # Set up or load the agent
    agent = setup_agent()
    
    # Add local tools to the agent
    agent.add_local_tools(local_tools_list)
    print("Local tools added to agent")
    
    # Add Tavily search capability
    tavily_ready = add_tavily_search(agent)
    
    # Have a conversation with the agent
    if tavily_ready:
        print("\nTesting agent with time function and search capability...")
        # First message - asking about time
        thread_id = chat(agent, "What time is it right now? Use the get_current_time tool.", 
                       local_tools_by_name=local_tools_by_name)
        
        # Second message in same thread - asking about search
        chat(agent, "What are the latest developments in AI?", thread_id, 
            local_tools_by_name=local_tools_by_name)
    else:
        print("\nTesting agent with time function only (search wasn't added successfully)...")
        # First message - asking about time
        thread_id = chat(agent, "What time is it right now? Use the get_current_time tool.", 
                       local_tools_by_name=local_tools_by_name)
        
        # Second message in same thread
        chat(agent, "Tell me more about time zones.", thread_id, 
            local_tools_by_name=local_tools_by_name)

# Run the example when the script is executed directly
if __name__ == "__main__":
    main()
```

Run the complete agent:

```bash
python complete_agent.py
```

## Tips for Success

- **Always save your agent ID** in your `.env` file
- **Use the `agent_loop` function** to avoid duplicate code and ensure proper monitoring
- **Monitor execution metrics** through the Xpander platform using the agent ID
- **Manage thread IDs** to maintain conversation context
- **Implement retry logic** for more reliable agent operations
- **Be aware of tool name conflicts** between local and built-in tools
- **Consider the UI** for visual graph creation and visualizing complex tool relationships

Remember that the Xpander SDK is a powerful foundation giving you direct programmatic control of your agents. The UI provides helpful visualization for agent graphs, but direct SDK access gives you maximum flexibility for custom integrations. 